{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Wisconsin breast cancer diagnostic data set for predictive analysis\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    " - 1) ID number \n",
    " - 2) Diagnosis (M = malignant, B = benign) \n",
    " \n",
    "-3-32.Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    " - a) radius (mean of distances from center to points on the perimeter) \n",
    " - b) texture (standard deviation of gray-scale values) \n",
    " - c) perimeter \n",
    " - d) area \n",
    " - e) smoothness (local variation in radius lengths) \n",
    " - f) compactness (perimeter^2 / area - 1.0) \n",
    " - g). concavity (severity of concave portions of the contour) \n",
    " - h). concave points (number of concave portions of the contour) \n",
    " - i). symmetry \n",
    " - j). fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# keeps the plots in one place. calls image as static pngs\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import matplotlib.gridspec as gridspec # subplots\n",
    "import seaborn as sns\n",
    "\n",
    "#Import models from scikit learn module:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean   \n",
       "0      842302         M        17.99         10.38          122.80     1001.0  \\\n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean   \n",
       "0            0.11840           0.27760         0.30010              0.14710  \\\n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst   \n",
       "0    ...          17.33           184.60      2019.0           0.16220  \\\n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst   \n",
       "0              0.66560           0.7119                0.2654          0.4601  \\\n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_df = pd.read_csv(\"data.csv\",header = 0)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the diagnosis is not done for some of the cases, the 'Nan' values will be present in those cases.\n",
    "```cpp\n",
    "raw_df.dropna(subset=['diagnosis'], inplace=True)\n",
    "```\n",
    "This command will remove all the rows where 'Nan' values in 'diagnosis' column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column doesnt have any value in it ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = raw_df.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.iloc[:, :-1] selects all rows (:) and all columns except the last one (:-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean   \n",
       "0      842302         M        17.99         10.38          122.80     1001.0  \\\n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean   \n",
       "0            0.11840           0.27760         0.30010              0.14710  \\\n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  radius_worst  texture_worst  perimeter_worst  area_worst   \n",
       "0    ...        25.380          17.33           184.60      2019.0  \\\n",
       "1    ...        24.990          23.41           158.80      1956.0   \n",
       "2    ...        23.570          25.53           152.50      1709.0   \n",
       "3    ...        14.910          26.50            98.87       567.7   \n",
       "4    ...        22.540          16.67           152.20      1575.0   \n",
       "..   ...           ...            ...              ...         ...   \n",
       "564  ...        25.450          26.40           166.10      2027.0   \n",
       "565  ...        23.690          38.25           155.00      1731.0   \n",
       "566  ...        18.980          34.12           126.70      1124.0   \n",
       "567  ...        25.740          39.42           184.60      1821.0   \n",
       "568  ...         9.456          30.37            59.16       268.6   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst   \n",
       "0             0.16220            0.66560           0.7119  \\\n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation and Test Sets\n",
    "\n",
    "While building real-world machine learning models, it is quite common to split the dataset into three parts:\n",
    "\n",
    "1. **Training set** - used to train the model, i.e., compute the loss and adjust the model's weights using an optimization technique. \n",
    "\n",
    "\n",
    "2. **Validation set** - used to evaluate the model during training, tune model hyperparameters (optimization technique, regularization etc.), and pick the best version of the model. Picking a good validation set is essential for training models that generalize well. [Learn more here.](https://www.fast.ai/2017/11/13/validation-sets/)\n",
    "\n",
    "\n",
    "3. **Test set** - used to compare different models or approaches and report the model's final accuracy. For many datasets, test sets are provided separately. The test set should reflect the kind of data the model will encounter in the real-world, as closely as feasible.\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/j8eITrK.png\" width=\"480\">\n",
    "\n",
    "\n",
    "As a general rule of thumb you can use around 60% of the data for the training set, 20% for the validation set and 20% for the test set. If a separate test set is already provided, you can use a 75%-25% training-validation split.\n",
    "\n",
    "\n",
    "When rows in the dataset have no inherent order, it's common practice to pick random subsets of rows for creating test and validation sets. This can be done using the `train_test_split` utility from `scikit-learn`. Learn more about it here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(raw_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape : (341, 32)\n",
      "val_df.shape : (114, 32)\n",
      "test_df.shape : (114, 32)\n"
     ]
    }
   ],
   "source": [
    "print('train_df.shape :', train_df.shape)\n",
    "print('val_df.shape :', val_df.shape)\n",
    "print('test_df.shape :', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = list(train_df.columns)[2:]\n",
    "target_col = 'diagnosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n"
     ]
    }
   ],
   "source": [
    "print(input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_df[input_cols].copy()\n",
    "train_targets = train_df[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs = val_df[input_cols].copy()\n",
    "val_targets = val_df[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = test_df[input_cols].copy()\n",
    "test_targets = test_df[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = train_inputs.select_dtypes('object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.001481</td>\n",
       "      <td>19.128152</td>\n",
       "      <td>91.030762</td>\n",
       "      <td>642.609971</td>\n",
       "      <td>0.095934</td>\n",
       "      <td>0.102269</td>\n",
       "      <td>0.085459</td>\n",
       "      <td>0.047155</td>\n",
       "      <td>0.182006</td>\n",
       "      <td>0.062750</td>\n",
       "      <td>...</td>\n",
       "      <td>16.125613</td>\n",
       "      <td>25.465279</td>\n",
       "      <td>106.273167</td>\n",
       "      <td>866.555132</td>\n",
       "      <td>0.131554</td>\n",
       "      <td>0.248931</td>\n",
       "      <td>0.264040</td>\n",
       "      <td>0.112563</td>\n",
       "      <td>0.291753</td>\n",
       "      <td>0.083672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.473378</td>\n",
       "      <td>4.197569</td>\n",
       "      <td>23.904744</td>\n",
       "      <td>345.479971</td>\n",
       "      <td>0.014450</td>\n",
       "      <td>0.053894</td>\n",
       "      <td>0.079140</td>\n",
       "      <td>0.038212</td>\n",
       "      <td>0.028181</td>\n",
       "      <td>0.007412</td>\n",
       "      <td>...</td>\n",
       "      <td>4.845435</td>\n",
       "      <td>5.977085</td>\n",
       "      <td>33.660162</td>\n",
       "      <td>578.566377</td>\n",
       "      <td>0.023416</td>\n",
       "      <td>0.157138</td>\n",
       "      <td>0.201125</td>\n",
       "      <td>0.066265</td>\n",
       "      <td>0.064154</td>\n",
       "      <td>0.018026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.691000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>47.920000</td>\n",
       "      <td>170.400000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>8.678000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>54.490000</td>\n",
       "      <td>223.600000</td>\n",
       "      <td>0.081250</td>\n",
       "      <td>0.034320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.630000</td>\n",
       "      <td>16.320000</td>\n",
       "      <td>74.680000</td>\n",
       "      <td>412.600000</td>\n",
       "      <td>0.085820</td>\n",
       "      <td>0.061590</td>\n",
       "      <td>0.026380</td>\n",
       "      <td>0.019240</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>0.057630</td>\n",
       "      <td>...</td>\n",
       "      <td>12.980000</td>\n",
       "      <td>21.180000</td>\n",
       "      <td>83.610000</td>\n",
       "      <td>513.900000</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>0.062960</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.070710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.200000</td>\n",
       "      <td>18.820000</td>\n",
       "      <td>85.480000</td>\n",
       "      <td>538.400000</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.057740</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.179900</td>\n",
       "      <td>0.061660</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910000</td>\n",
       "      <td>25.220000</td>\n",
       "      <td>97.170000</td>\n",
       "      <td>674.700000</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.205700</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.098610</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.079610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.660000</td>\n",
       "      <td>21.560000</td>\n",
       "      <td>102.800000</td>\n",
       "      <td>758.600000</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.067720</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.066010</td>\n",
       "      <td>...</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>29.410000</td>\n",
       "      <td>122.400000</td>\n",
       "      <td>1021.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.341600</td>\n",
       "      <td>0.375900</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.317600</td>\n",
       "      <td>0.092060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27.420000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>186.900000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.311400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.218400</td>\n",
       "      <td>0.937900</td>\n",
       "      <td>0.960800</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.173000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       radius_mean  texture_mean  perimeter_mean    area_mean   \n",
       "count   341.000000    341.000000      341.000000   341.000000  \\\n",
       "mean     14.001481     19.128152       91.030762   642.609971   \n",
       "std       3.473378      4.197569       23.904744   345.479971   \n",
       "min       7.691000      9.710000       47.920000   170.400000   \n",
       "25%      11.630000     16.320000       74.680000   412.600000   \n",
       "50%      13.200000     18.820000       85.480000   538.400000   \n",
       "75%      15.660000     21.560000      102.800000   758.600000   \n",
       "max      27.420000     39.280000      186.900000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean   \n",
       "count       341.000000        341.000000      341.000000           341.000000  \\\n",
       "mean          0.095934          0.102269        0.085459             0.047155   \n",
       "std           0.014450          0.053894        0.079140             0.038212   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.085820          0.061590        0.026380             0.019240   \n",
       "50%           0.094400          0.086420        0.057740             0.032500   \n",
       "75%           0.104900          0.129900        0.121800             0.067720   \n",
       "max           0.163400          0.311400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  fractal_dimension_mean  ...  radius_worst   \n",
       "count     341.000000              341.000000  ...    341.000000  \\\n",
       "mean        0.182006                0.062750  ...     16.125613   \n",
       "std         0.028181                0.007412  ...      4.845435   \n",
       "min         0.116700                0.049960  ...      8.678000   \n",
       "25%         0.162000                0.057630  ...     12.980000   \n",
       "50%         0.179900                0.061660  ...     14.910000   \n",
       "75%         0.196600                0.066010  ...     18.100000   \n",
       "max         0.274300                0.097440  ...     36.040000   \n",
       "\n",
       "       texture_worst  perimeter_worst   area_worst  smoothness_worst   \n",
       "count     341.000000       341.000000   341.000000        341.000000  \\\n",
       "mean       25.465279       106.273167   866.555132          0.131554   \n",
       "std         5.977085        33.660162   578.566377          0.023416   \n",
       "min        12.020000        54.490000   223.600000          0.081250   \n",
       "25%        21.180000        83.610000   513.900000          0.114000   \n",
       "50%        25.220000        97.170000   674.700000          0.130300   \n",
       "75%        29.410000       122.400000  1021.000000          0.146000   \n",
       "max        49.540000       251.200000  4254.000000          0.218400   \n",
       "\n",
       "       compactness_worst  concavity_worst  concave points_worst   \n",
       "count         341.000000       341.000000            341.000000  \\\n",
       "mean            0.248931         0.264040              0.112563   \n",
       "std             0.157138         0.201125              0.066265   \n",
       "min             0.034320         0.000000              0.000000   \n",
       "25%             0.138100         0.108700              0.062960   \n",
       "50%             0.205700         0.215100              0.098610   \n",
       "75%             0.341600         0.375900              0.156500   \n",
       "max             0.937900         0.960800              0.291000   \n",
       "\n",
       "       symmetry_worst  fractal_dimension_worst  \n",
       "count      341.000000               341.000000  \n",
       "mean         0.291753                 0.083672  \n",
       "std          0.064154                 0.018026  \n",
       "min          0.156500                 0.055040  \n",
       "25%          0.252500                 0.070710  \n",
       "50%          0.284100                 0.079610  \n",
       "75%          0.317600                 0.092060  \n",
       "max          0.663800                 0.173000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[categorical_cols].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Numeric Data\n",
    "\n",
    "Machine learning models can't work with missing numerical data. The process of filling missing values is called imputation.\n",
    "\n",
    "<img src=\"https://i.imgur.com/W7cfyOp.png\" width=\"480\">\n",
    "\n",
    "There are several techniques for imputation, but we'll use the most basic one: replacing missing values with the average value in the column using the `SimpleImputer` class from `sklearn.impute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SimpleImputer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SimpleImputer()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.fit(raw_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.127291739894552,\n",
       " 19.289648506151142,\n",
       " 91.96903339191564,\n",
       " 654.8891036906855,\n",
       " 0.0963602811950791,\n",
       " 0.10434098418277679,\n",
       " 0.0887993158172232,\n",
       " 0.04891914586994728,\n",
       " 0.18116186291739894,\n",
       " 0.06279760984182776,\n",
       " 0.40517205623901575,\n",
       " 1.2168534270650264,\n",
       " 2.8660592267135327,\n",
       " 40.337079086116,\n",
       " 0.007040978910369069,\n",
       " 0.025478138840070295,\n",
       " 0.03189371634446397,\n",
       " 0.011796137082601054,\n",
       " 0.02054229876977153,\n",
       " 0.0037949038664323374,\n",
       " 16.269189806678387,\n",
       " 25.677223198594024,\n",
       " 107.26121265377857,\n",
       " 880.5831282952548,\n",
       " 0.13236859402460457,\n",
       " 0.25426504393673116,\n",
       " 0.27218848330404216,\n",
       " 0.11460622319859401,\n",
       " 0.2900755711775044,\n",
       " 0.0839458172231986]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(imputer.statistics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols])\n",
    "val_inputs[numeric_cols] = imputer.transform(val_inputs[numeric_cols])\n",
    "test_inputs[numeric_cols] = imputer.transform(test_inputs[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Numeric Features\n",
    "\n",
    "Another good practice is to scale numeric features to a small range of values e.g. $(0,1)$ or $(-1,1)$. Scaling numeric features ensures that no particular feature has a disproportionate impact on the model's loss. Optimization algorithms also work better in practice with smaller numbers.\n",
    "\n",
    "The numeric columns in our dataset have varying ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       radius_mean  texture_mean  perimeter_mean    area_mean   \n",
       "count   569.000000    569.000000      569.000000   569.000000  \\\n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean   \n",
       "count       569.000000        569.000000      569.000000           569.000000  \\\n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  fractal_dimension_mean  ...  radius_worst   \n",
       "count     569.000000              569.000000  ...    569.000000  \\\n",
       "mean        0.181162                0.062798  ...     16.269190   \n",
       "std         0.027414                0.007060  ...      4.833242   \n",
       "min         0.106000                0.049960  ...      7.930000   \n",
       "25%         0.161900                0.057700  ...     13.010000   \n",
       "50%         0.179200                0.061540  ...     14.970000   \n",
       "75%         0.195700                0.066120  ...     18.790000   \n",
       "max         0.304000                0.097440  ...     36.040000   \n",
       "\n",
       "       texture_worst  perimeter_worst   area_worst  smoothness_worst   \n",
       "count     569.000000       569.000000   569.000000        569.000000  \\\n",
       "mean       25.677223       107.261213   880.583128          0.132369   \n",
       "std         6.146258        33.602542   569.356993          0.022832   \n",
       "min        12.020000        50.410000   185.200000          0.071170   \n",
       "25%        21.080000        84.110000   515.300000          0.116600   \n",
       "50%        25.410000        97.660000   686.500000          0.131300   \n",
       "75%        29.720000       125.400000  1084.000000          0.146000   \n",
       "max        49.540000       251.200000  4254.000000          0.222600   \n",
       "\n",
       "       compactness_worst  concavity_worst  concave points_worst   \n",
       "count         569.000000       569.000000            569.000000  \\\n",
       "mean            0.254265         0.272188              0.114606   \n",
       "std             0.157336         0.208624              0.065732   \n",
       "min             0.027290         0.000000              0.000000   \n",
       "25%             0.147200         0.114500              0.064930   \n",
       "50%             0.211900         0.226700              0.099930   \n",
       "75%             0.339100         0.382900              0.161400   \n",
       "max             1.058000         1.252000              0.291000   \n",
       "\n",
       "       symmetry_worst  fractal_dimension_worst  \n",
       "count      569.000000               569.000000  \n",
       "mean         0.290076                 0.083946  \n",
       "std          0.061867                 0.018061  \n",
       "min          0.156500                 0.055040  \n",
       "25%          0.250400                 0.071460  \n",
       "50%          0.282200                 0.080040  \n",
       "75%          0.317900                 0.092080  \n",
       "max          0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use `MinMaxScaler` from `sklearn.preprocessing` to scale values to the $(0,1)$ range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Transform features by scaling each feature to a given range.\n",
       "\n",
       "This estimator scales and translates each feature individually such\n",
       "that it is in the given range on the training set, e.g. between\n",
       "zero and one.\n",
       "\n",
       "The transformation is given by::\n",
       "\n",
       "    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
       "    X_scaled = X_std * (max - min) + min\n",
       "\n",
       "where min, max = feature_range.\n",
       "\n",
       "This transformation is often used as an alternative to zero mean,\n",
       "unit variance scaling.\n",
       "\n",
       "Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "feature_range : tuple (min, max), default=(0, 1)\n",
       "    Desired range of transformed data.\n",
       "\n",
       "copy : bool, default=True\n",
       "    Set to False to perform inplace row normalization and avoid a\n",
       "    copy (if the input is already a numpy array).\n",
       "\n",
       "clip : bool, default=False\n",
       "    Set to True to clip transformed values of held-out data to\n",
       "    provided `feature range`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "min_ : ndarray of shape (n_features,)\n",
       "    Per feature adjustment for minimum. Equivalent to\n",
       "    ``min - X.min(axis=0) * self.scale_``\n",
       "\n",
       "scale_ : ndarray of shape (n_features,)\n",
       "    Per feature relative scaling of the data. Equivalent to\n",
       "    ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *scale_* attribute.\n",
       "\n",
       "data_min_ : ndarray of shape (n_features,)\n",
       "    Per feature minimum seen in the data\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *data_min_*\n",
       "\n",
       "data_max_ : ndarray of shape (n_features,)\n",
       "    Per feature maximum seen in the data\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *data_max_*\n",
       "\n",
       "data_range_ : ndarray of shape (n_features,)\n",
       "    Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *data_range_*\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "n_samples_seen_ : int\n",
       "    The number of samples processed by the estimator.\n",
       "    It will be reset on new calls to fit, but increments across\n",
       "    ``partial_fit`` calls.\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "minmax_scale : Equivalent function without the estimator API.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "NaNs are treated as missing values: disregarded in fit, and maintained in\n",
       "transform.\n",
       "\n",
       "For a comparison of the different scalers, transformers, and normalizers,\n",
       "see :ref:`examples/preprocessing/plot_all_scaling.py\n",
       "<sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.preprocessing import MinMaxScaler\n",
       ">>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
       ">>> scaler = MinMaxScaler()\n",
       ">>> print(scaler.fit(data))\n",
       "MinMaxScaler()\n",
       ">>> print(scaler.data_max_)\n",
       "[ 1. 18.]\n",
       ">>> print(scaler.transform(data))\n",
       "[[0.   0.  ]\n",
       " [0.25 0.25]\n",
       " [0.5  0.5 ]\n",
       " [1.   1.  ]]\n",
       ">>> print(scaler.transform([[2, 2]]))\n",
       "[[1.5 0. ]]\n",
       "\u001b[0;31mFile:\u001b[0m           ~/products/conda/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(raw_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.981,\n",
       " 9.71,\n",
       " 43.79,\n",
       " 143.5,\n",
       " 0.05263,\n",
       " 0.01938,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.106,\n",
       " 0.04996,\n",
       " 0.1115,\n",
       " 0.3602,\n",
       " 0.757,\n",
       " 6.802,\n",
       " 0.001713,\n",
       " 0.002252,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.007882,\n",
       " 0.0008948,\n",
       " 7.93,\n",
       " 12.02,\n",
       " 50.41,\n",
       " 185.2,\n",
       " 0.07117,\n",
       " 0.02729,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1565,\n",
       " 0.05504]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Minimum:')\n",
    "list(scaler.data_min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[28.11,\n",
       " 39.28,\n",
       " 188.5,\n",
       " 2501.0,\n",
       " 0.1634,\n",
       " 0.3454,\n",
       " 0.4268,\n",
       " 0.2012,\n",
       " 0.304,\n",
       " 0.09744,\n",
       " 2.873,\n",
       " 4.885,\n",
       " 21.98,\n",
       " 542.2,\n",
       " 0.03113,\n",
       " 0.1354,\n",
       " 0.396,\n",
       " 0.05279,\n",
       " 0.07895,\n",
       " 0.02984,\n",
       " 36.04,\n",
       " 49.54,\n",
       " 251.2,\n",
       " 4254.0,\n",
       " 0.2226,\n",
       " 1.058,\n",
       " 1.252,\n",
       " 0.291,\n",
       " 0.6638,\n",
       " 0.2075]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Maximum:')\n",
    "list(scaler.data_max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols])\n",
    "val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols])\n",
    "test_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>341.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.332268</td>\n",
       "      <td>0.318504</td>\n",
       "      <td>0.326451</td>\n",
       "      <td>0.211712</td>\n",
       "      <td>0.390940</td>\n",
       "      <td>0.254246</td>\n",
       "      <td>0.200232</td>\n",
       "      <td>0.234369</td>\n",
       "      <td>0.383871</td>\n",
       "      <td>0.269382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291555</td>\n",
       "      <td>0.358350</td>\n",
       "      <td>0.278217</td>\n",
       "      <td>0.167458</td>\n",
       "      <td>0.398758</td>\n",
       "      <td>0.215037</td>\n",
       "      <td>0.210895</td>\n",
       "      <td>0.386814</td>\n",
       "      <td>0.266614</td>\n",
       "      <td>0.187798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.164389</td>\n",
       "      <td>0.141954</td>\n",
       "      <td>0.165191</td>\n",
       "      <td>0.146545</td>\n",
       "      <td>0.130450</td>\n",
       "      <td>0.165308</td>\n",
       "      <td>0.185427</td>\n",
       "      <td>0.189919</td>\n",
       "      <td>0.142330</td>\n",
       "      <td>0.156107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172374</td>\n",
       "      <td>0.159304</td>\n",
       "      <td>0.167639</td>\n",
       "      <td>0.142196</td>\n",
       "      <td>0.154633</td>\n",
       "      <td>0.152456</td>\n",
       "      <td>0.160643</td>\n",
       "      <td>0.227713</td>\n",
       "      <td>0.126462</td>\n",
       "      <td>0.118234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.033603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028540</td>\n",
       "      <td>0.011410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020320</td>\n",
       "      <td>0.009438</td>\n",
       "      <td>0.066565</td>\n",
       "      <td>0.006821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.220029</td>\n",
       "      <td>0.223537</td>\n",
       "      <td>0.213461</td>\n",
       "      <td>0.114146</td>\n",
       "      <td>0.299630</td>\n",
       "      <td>0.129471</td>\n",
       "      <td>0.061809</td>\n",
       "      <td>0.095626</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.161542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179651</td>\n",
       "      <td>0.244136</td>\n",
       "      <td>0.165347</td>\n",
       "      <td>0.080785</td>\n",
       "      <td>0.282837</td>\n",
       "      <td>0.107508</td>\n",
       "      <td>0.086821</td>\n",
       "      <td>0.216357</td>\n",
       "      <td>0.189237</td>\n",
       "      <td>0.102781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.294335</td>\n",
       "      <td>0.308083</td>\n",
       "      <td>0.288093</td>\n",
       "      <td>0.167508</td>\n",
       "      <td>0.377088</td>\n",
       "      <td>0.205632</td>\n",
       "      <td>0.135286</td>\n",
       "      <td>0.161531</td>\n",
       "      <td>0.373232</td>\n",
       "      <td>0.246420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248310</td>\n",
       "      <td>0.351812</td>\n",
       "      <td>0.232880</td>\n",
       "      <td>0.120306</td>\n",
       "      <td>0.390477</td>\n",
       "      <td>0.173094</td>\n",
       "      <td>0.171805</td>\n",
       "      <td>0.338866</td>\n",
       "      <td>0.251528</td>\n",
       "      <td>0.161157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.410762</td>\n",
       "      <td>0.400744</td>\n",
       "      <td>0.407781</td>\n",
       "      <td>0.260912</td>\n",
       "      <td>0.471879</td>\n",
       "      <td>0.338998</td>\n",
       "      <td>0.285380</td>\n",
       "      <td>0.336581</td>\n",
       "      <td>0.457576</td>\n",
       "      <td>0.338037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361793</td>\n",
       "      <td>0.463486</td>\n",
       "      <td>0.358534</td>\n",
       "      <td>0.205417</td>\n",
       "      <td>0.494156</td>\n",
       "      <td>0.304945</td>\n",
       "      <td>0.300240</td>\n",
       "      <td>0.537801</td>\n",
       "      <td>0.317564</td>\n",
       "      <td>0.242818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.967343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988943</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972264</td>\n",
       "      <td>0.883478</td>\n",
       "      <td>0.767412</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       radius_mean  texture_mean  perimeter_mean   area_mean  smoothness_mean   \n",
       "count   341.000000    341.000000      341.000000  341.000000       341.000000  \\\n",
       "mean      0.332268      0.318504        0.326451    0.211712         0.390940   \n",
       "std       0.164389      0.141954        0.165191    0.146545         0.130450   \n",
       "min       0.033603      0.000000        0.028540    0.011410         0.000000   \n",
       "25%       0.220029      0.223537        0.213461    0.114146         0.299630   \n",
       "50%       0.294335      0.308083        0.288093    0.167508         0.377088   \n",
       "75%       0.410762      0.400744        0.407781    0.260912         0.471879   \n",
       "max       0.967343      1.000000        0.988943    1.000000         1.000000   \n",
       "\n",
       "       compactness_mean  concavity_mean  concave points_mean  symmetry_mean   \n",
       "count        341.000000      341.000000           341.000000     341.000000  \\\n",
       "mean           0.254246        0.200232             0.234369       0.383871   \n",
       "std            0.165308        0.185427             0.189919       0.142330   \n",
       "min            0.000000        0.000000             0.000000       0.054040   \n",
       "25%            0.129471        0.061809             0.095626       0.282828   \n",
       "50%            0.205632        0.135286             0.161531       0.373232   \n",
       "75%            0.338998        0.285380             0.336581       0.457576   \n",
       "max            0.895712        1.000000             1.000000       0.850000   \n",
       "\n",
       "       fractal_dimension_mean  ...  radius_worst  texture_worst   \n",
       "count              341.000000  ...    341.000000     341.000000  \\\n",
       "mean                 0.269382  ...      0.291555       0.358350   \n",
       "std                  0.156107  ...      0.172374       0.159304   \n",
       "min                  0.000000  ...      0.026610       0.000000   \n",
       "25%                  0.161542  ...      0.179651       0.244136   \n",
       "50%                  0.246420  ...      0.248310       0.351812   \n",
       "75%                  0.338037  ...      0.361793       0.463486   \n",
       "max                  1.000000  ...      1.000000       1.000000   \n",
       "\n",
       "       perimeter_worst  area_worst  smoothness_worst  compactness_worst   \n",
       "count       341.000000  341.000000        341.000000         341.000000  \\\n",
       "mean          0.278217    0.167458          0.398758           0.215037   \n",
       "std           0.167639    0.142196          0.154633           0.152456   \n",
       "min           0.020320    0.009438          0.066565           0.006821   \n",
       "25%           0.165347    0.080785          0.282837           0.107508   \n",
       "50%           0.232880    0.120306          0.390477           0.173094   \n",
       "75%           0.358534    0.205417          0.494156           0.304945   \n",
       "max           1.000000    1.000000          0.972264           0.883478   \n",
       "\n",
       "       concavity_worst  concave points_worst  symmetry_worst   \n",
       "count       341.000000            341.000000      341.000000  \\\n",
       "mean          0.210895              0.386814        0.266614   \n",
       "std           0.160643              0.227713        0.126462   \n",
       "min           0.000000              0.000000        0.000000   \n",
       "25%           0.086821              0.216357        0.189237   \n",
       "50%           0.171805              0.338866        0.251528   \n",
       "75%           0.300240              0.537801        0.317564   \n",
       "max           0.767412              1.000000        1.000000   \n",
       "\n",
       "       fractal_dimension_worst  \n",
       "count               341.000000  \n",
       "mean                  0.187798  \n",
       "std                   0.118234  \n",
       "min                   0.000000  \n",
       "25%                   0.102781  \n",
       "50%                   0.161157  \n",
       "75%                   0.242818  \n",
       "max                   0.773711  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data\n",
    "\n",
    "Since machine learning models can only be trained with numeric data, we need to convert categorical data to numbers. A common technique is to use one-hot encoding for categorical columns.\n",
    "\n",
    "<img src=\"https://i.imgur.com/n8GuiOO.png\" width=\"640\">\n",
    "\n",
    "One hot encoding involves adding a new binary (0/1) column for each unique category of a categorical column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/products/conda/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoder.fit(raw_df[categorical_cols])\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
    "#val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
    "#test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.259785</td>\n",
       "      <td>0.300643</td>\n",
       "      <td>0.257757</td>\n",
       "      <td>0.143542</td>\n",
       "      <td>0.424483</td>\n",
       "      <td>0.265076</td>\n",
       "      <td>0.187559</td>\n",
       "      <td>0.189911</td>\n",
       "      <td>0.436869</td>\n",
       "      <td>0.290017</td>\n",
       "      <td>0.103060</td>\n",
       "      <td>0.151123</td>\n",
       "      <td>0.081987</td>\n",
       "      <td>0.043870</td>\n",
       "      <td>0.178128</td>\n",
       "      <td>0.126611</td>\n",
       "      <td>0.068207</td>\n",
       "      <td>0.196439</td>\n",
       "      <td>0.139838</td>\n",
       "      <td>0.092976</td>\n",
       "      <td>0.250445</td>\n",
       "      <td>0.336354</td>\n",
       "      <td>0.227302</td>\n",
       "      <td>0.121092</td>\n",
       "      <td>0.471703</td>\n",
       "      <td>0.204238</td>\n",
       "      <td>0.213339</td>\n",
       "      <td>0.348797</td>\n",
       "      <td>0.285630</td>\n",
       "      <td>0.212908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.565999</td>\n",
       "      <td>0.392289</td>\n",
       "      <td>0.551517</td>\n",
       "      <td>0.418452</td>\n",
       "      <td>0.338178</td>\n",
       "      <td>0.256181</td>\n",
       "      <td>0.253046</td>\n",
       "      <td>0.395179</td>\n",
       "      <td>0.263636</td>\n",
       "      <td>0.097936</td>\n",
       "      <td>0.245265</td>\n",
       "      <td>0.096645</td>\n",
       "      <td>0.222824</td>\n",
       "      <td>0.166695</td>\n",
       "      <td>0.092837</td>\n",
       "      <td>0.107159</td>\n",
       "      <td>0.057298</td>\n",
       "      <td>0.259519</td>\n",
       "      <td>0.084117</td>\n",
       "      <td>0.027749</td>\n",
       "      <td>0.602277</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.575178</td>\n",
       "      <td>0.413095</td>\n",
       "      <td>0.317837</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.214617</td>\n",
       "      <td>0.614777</td>\n",
       "      <td>0.194362</td>\n",
       "      <td>0.071166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.401297</td>\n",
       "      <td>0.330402</td>\n",
       "      <td>0.400180</td>\n",
       "      <td>0.256797</td>\n",
       "      <td>0.510698</td>\n",
       "      <td>0.315686</td>\n",
       "      <td>0.343486</td>\n",
       "      <td>0.401938</td>\n",
       "      <td>0.439899</td>\n",
       "      <td>0.168492</td>\n",
       "      <td>0.131378</td>\n",
       "      <td>0.094082</td>\n",
       "      <td>0.110116</td>\n",
       "      <td>0.077527</td>\n",
       "      <td>0.153891</td>\n",
       "      <td>0.094541</td>\n",
       "      <td>0.071035</td>\n",
       "      <td>0.207047</td>\n",
       "      <td>0.085664</td>\n",
       "      <td>0.054109</td>\n",
       "      <td>0.403059</td>\n",
       "      <td>0.372601</td>\n",
       "      <td>0.370985</td>\n",
       "      <td>0.238596</td>\n",
       "      <td>0.550948</td>\n",
       "      <td>0.205790</td>\n",
       "      <td>0.302796</td>\n",
       "      <td>0.520275</td>\n",
       "      <td>0.250739</td>\n",
       "      <td>0.164961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.256472</td>\n",
       "      <td>0.269530</td>\n",
       "      <td>0.260383</td>\n",
       "      <td>0.137561</td>\n",
       "      <td>0.476393</td>\n",
       "      <td>0.344212</td>\n",
       "      <td>0.181373</td>\n",
       "      <td>0.139115</td>\n",
       "      <td>0.379293</td>\n",
       "      <td>0.443555</td>\n",
       "      <td>0.023610</td>\n",
       "      <td>0.243060</td>\n",
       "      <td>0.068181</td>\n",
       "      <td>0.016115</td>\n",
       "      <td>0.281708</td>\n",
       "      <td>0.230555</td>\n",
       "      <td>0.122753</td>\n",
       "      <td>0.221065</td>\n",
       "      <td>0.196826</td>\n",
       "      <td>0.176547</td>\n",
       "      <td>0.176094</td>\n",
       "      <td>0.290245</td>\n",
       "      <td>0.195229</td>\n",
       "      <td>0.081252</td>\n",
       "      <td>0.487552</td>\n",
       "      <td>0.228590</td>\n",
       "      <td>0.191933</td>\n",
       "      <td>0.253265</td>\n",
       "      <td>0.195348</td>\n",
       "      <td>0.252853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>0.215770</td>\n",
       "      <td>0.159959</td>\n",
       "      <td>0.213254</td>\n",
       "      <td>0.110032</td>\n",
       "      <td>0.426198</td>\n",
       "      <td>0.284093</td>\n",
       "      <td>0.157849</td>\n",
       "      <td>0.128926</td>\n",
       "      <td>0.382828</td>\n",
       "      <td>0.376158</td>\n",
       "      <td>0.060438</td>\n",
       "      <td>0.311130</td>\n",
       "      <td>0.041040</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>0.354795</td>\n",
       "      <td>0.291916</td>\n",
       "      <td>0.140227</td>\n",
       "      <td>0.283008</td>\n",
       "      <td>0.147999</td>\n",
       "      <td>0.159515</td>\n",
       "      <td>0.154038</td>\n",
       "      <td>0.204158</td>\n",
       "      <td>0.141292</td>\n",
       "      <td>0.066998</td>\n",
       "      <td>0.418213</td>\n",
       "      <td>0.179013</td>\n",
       "      <td>0.143530</td>\n",
       "      <td>0.237732</td>\n",
       "      <td>0.150601</td>\n",
       "      <td>0.172504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.362488</td>\n",
       "      <td>0.241461</td>\n",
       "      <td>0.348421</td>\n",
       "      <td>0.221633</td>\n",
       "      <td>0.304956</td>\n",
       "      <td>0.146003</td>\n",
       "      <td>0.121649</td>\n",
       "      <td>0.138718</td>\n",
       "      <td>0.176263</td>\n",
       "      <td>0.075611</td>\n",
       "      <td>0.039435</td>\n",
       "      <td>0.142725</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.024613</td>\n",
       "      <td>0.061937</td>\n",
       "      <td>0.087707</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.116386</td>\n",
       "      <td>0.065543</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.303451</td>\n",
       "      <td>0.357676</td>\n",
       "      <td>0.276856</td>\n",
       "      <td>0.158720</td>\n",
       "      <td>0.284158</td>\n",
       "      <td>0.174356</td>\n",
       "      <td>0.194649</td>\n",
       "      <td>0.269003</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.071625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.430167</td>\n",
       "      <td>0.336152</td>\n",
       "      <td>0.416765</td>\n",
       "      <td>0.285981</td>\n",
       "      <td>0.352532</td>\n",
       "      <td>0.198945</td>\n",
       "      <td>0.228889</td>\n",
       "      <td>0.329920</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>0.083193</td>\n",
       "      <td>0.230273</td>\n",
       "      <td>0.144935</td>\n",
       "      <td>0.201291</td>\n",
       "      <td>0.135316</td>\n",
       "      <td>0.309583</td>\n",
       "      <td>0.148541</td>\n",
       "      <td>0.088384</td>\n",
       "      <td>0.342679</td>\n",
       "      <td>0.107193</td>\n",
       "      <td>0.036386</td>\n",
       "      <td>0.421202</td>\n",
       "      <td>0.334222</td>\n",
       "      <td>0.390408</td>\n",
       "      <td>0.255063</td>\n",
       "      <td>0.520571</td>\n",
       "      <td>0.171930</td>\n",
       "      <td>0.225958</td>\n",
       "      <td>0.522337</td>\n",
       "      <td>0.213877</td>\n",
       "      <td>0.057917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.214823</td>\n",
       "      <td>0.176530</td>\n",
       "      <td>0.207864</td>\n",
       "      <td>0.111474</td>\n",
       "      <td>0.439379</td>\n",
       "      <td>0.180050</td>\n",
       "      <td>0.101406</td>\n",
       "      <td>0.145577</td>\n",
       "      <td>0.415657</td>\n",
       "      <td>0.246841</td>\n",
       "      <td>0.052399</td>\n",
       "      <td>0.149797</td>\n",
       "      <td>0.043773</td>\n",
       "      <td>0.022073</td>\n",
       "      <td>0.168236</td>\n",
       "      <td>0.075315</td>\n",
       "      <td>0.053157</td>\n",
       "      <td>0.190566</td>\n",
       "      <td>0.125063</td>\n",
       "      <td>0.065268</td>\n",
       "      <td>0.167912</td>\n",
       "      <td>0.244403</td>\n",
       "      <td>0.151751</td>\n",
       "      <td>0.075354</td>\n",
       "      <td>0.447269</td>\n",
       "      <td>0.127010</td>\n",
       "      <td>0.144089</td>\n",
       "      <td>0.330172</td>\n",
       "      <td>0.216637</td>\n",
       "      <td>0.151187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.342610</td>\n",
       "      <td>0.613460</td>\n",
       "      <td>0.336950</td>\n",
       "      <td>0.203775</td>\n",
       "      <td>0.267220</td>\n",
       "      <td>0.259248</td>\n",
       "      <td>0.258435</td>\n",
       "      <td>0.219085</td>\n",
       "      <td>0.142424</td>\n",
       "      <td>0.238627</td>\n",
       "      <td>0.081079</td>\n",
       "      <td>0.434008</td>\n",
       "      <td>0.063516</td>\n",
       "      <td>0.043254</td>\n",
       "      <td>0.156168</td>\n",
       "      <td>0.196758</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.191514</td>\n",
       "      <td>0.055834</td>\n",
       "      <td>0.093079</td>\n",
       "      <td>0.278193</td>\n",
       "      <td>0.760128</td>\n",
       "      <td>0.259425</td>\n",
       "      <td>0.142253</td>\n",
       "      <td>0.243875</td>\n",
       "      <td>0.208895</td>\n",
       "      <td>0.244728</td>\n",
       "      <td>0.282440</td>\n",
       "      <td>0.064065</td>\n",
       "      <td>0.150335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.650717</td>\n",
       "      <td>0.724045</td>\n",
       "      <td>0.635132</td>\n",
       "      <td>0.541039</td>\n",
       "      <td>0.379706</td>\n",
       "      <td>0.291148</td>\n",
       "      <td>0.320291</td>\n",
       "      <td>0.429722</td>\n",
       "      <td>0.358081</td>\n",
       "      <td>0.142797</td>\n",
       "      <td>0.384030</td>\n",
       "      <td>0.277758</td>\n",
       "      <td>0.329454</td>\n",
       "      <td>0.360289</td>\n",
       "      <td>0.096475</td>\n",
       "      <td>0.094091</td>\n",
       "      <td>0.054116</td>\n",
       "      <td>0.175791</td>\n",
       "      <td>0.081443</td>\n",
       "      <td>0.048512</td>\n",
       "      <td>0.873710</td>\n",
       "      <td>0.936567</td>\n",
       "      <td>0.814732</td>\n",
       "      <td>0.797975</td>\n",
       "      <td>0.455194</td>\n",
       "      <td>0.230045</td>\n",
       "      <td>0.274920</td>\n",
       "      <td>0.570103</td>\n",
       "      <td>0.256850</td>\n",
       "      <td>0.178014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean   \n",
       "204     0.259785      0.300643        0.257757   0.143542         0.424483  \\\n",
       "70      0.565999      0.392289        0.551517   0.418452         0.338178   \n",
       "131     0.401297      0.330402        0.400180   0.256797         0.510698   \n",
       "431     0.256472      0.269530        0.260383   0.137561         0.476393   \n",
       "540     0.215770      0.159959        0.213254   0.110032         0.426198   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "486     0.362488      0.241461        0.348421   0.221633         0.304956   \n",
       "75      0.430167      0.336152        0.416765   0.285981         0.352532   \n",
       "249     0.214823      0.176530        0.207864   0.111474         0.439379   \n",
       "238     0.342610      0.613460        0.336950   0.203775         0.267220   \n",
       "265     0.650717      0.724045        0.635132   0.541039         0.379706   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean   \n",
       "204          0.265076        0.187559             0.189911       0.436869  \\\n",
       "70           0.256181        0.253046             0.395179       0.263636   \n",
       "131          0.315686        0.343486             0.401938       0.439899   \n",
       "431          0.344212        0.181373             0.139115       0.379293   \n",
       "540          0.284093        0.157849             0.128926       0.382828   \n",
       "..                ...             ...                  ...            ...   \n",
       "486          0.146003        0.121649             0.138718       0.176263   \n",
       "75           0.198945        0.228889             0.329920       0.372727   \n",
       "249          0.180050        0.101406             0.145577       0.415657   \n",
       "238          0.259248        0.258435             0.219085       0.142424   \n",
       "265          0.291148        0.320291             0.429722       0.358081   \n",
       "\n",
       "     fractal_dimension_mean  radius_se  texture_se  perimeter_se   area_se   \n",
       "204                0.290017   0.103060    0.151123      0.081987  0.043870  \\\n",
       "70                 0.097936   0.245265    0.096645      0.222824  0.166695   \n",
       "131                0.168492   0.131378    0.094082      0.110116  0.077527   \n",
       "431                0.443555   0.023610    0.243060      0.068181  0.016115   \n",
       "540                0.376158   0.060438    0.311130      0.041040  0.026257   \n",
       "..                      ...        ...         ...           ...       ...   \n",
       "486                0.075611   0.039435    0.142725      0.033643  0.024613   \n",
       "75                 0.083193   0.230273    0.144935      0.201291  0.135316   \n",
       "249                0.246841   0.052399    0.149797      0.043773  0.022073   \n",
       "238                0.238627   0.081079    0.434008      0.063516  0.043254   \n",
       "265                0.142797   0.384030    0.277758      0.329454  0.360289   \n",
       "\n",
       "     smoothness_se  compactness_se  concavity_se  concave points_se   \n",
       "204       0.178128        0.126611      0.068207           0.196439  \\\n",
       "70        0.092837        0.107159      0.057298           0.259519   \n",
       "131       0.153891        0.094541      0.071035           0.207047   \n",
       "431       0.281708        0.230555      0.122753           0.221065   \n",
       "540       0.354795        0.291916      0.140227           0.283008   \n",
       "..             ...             ...           ...                ...   \n",
       "486       0.061937        0.087707      0.045455           0.116386   \n",
       "75        0.309583        0.148541      0.088384           0.342679   \n",
       "249       0.168236        0.075315      0.053157           0.190566   \n",
       "238       0.156168        0.196758      0.097222           0.191514   \n",
       "265       0.096475        0.094091      0.054116           0.175791   \n",
       "\n",
       "     symmetry_se  fractal_dimension_se  radius_worst  texture_worst   \n",
       "204     0.139838              0.092976      0.250445       0.336354  \\\n",
       "70      0.084117              0.027749      0.602277       0.388060   \n",
       "131     0.085664              0.054109      0.403059       0.372601   \n",
       "431     0.196826              0.176547      0.176094       0.290245   \n",
       "540     0.147999              0.159515      0.154038       0.204158   \n",
       "..           ...                   ...           ...            ...   \n",
       "486     0.065543              0.011200      0.303451       0.357676   \n",
       "75      0.107193              0.036386      0.421202       0.334222   \n",
       "249     0.125063              0.065268      0.167912       0.244403   \n",
       "238     0.055834              0.093079      0.278193       0.760128   \n",
       "265     0.081443              0.048512      0.873710       0.936567   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst   \n",
       "204         0.227302    0.121092          0.471703           0.204238  \\\n",
       "70          0.575178    0.413095          0.317837           0.200163   \n",
       "131         0.370985    0.238596          0.550948           0.205790   \n",
       "431         0.195229    0.081252          0.487552           0.228590   \n",
       "540         0.141292    0.066998          0.418213           0.179013   \n",
       "..               ...         ...               ...                ...   \n",
       "486         0.276856    0.158720          0.284158           0.174356   \n",
       "75          0.390408    0.255063          0.520571           0.171930   \n",
       "249         0.151751    0.075354          0.447269           0.127010   \n",
       "238         0.259425    0.142253          0.243875           0.208895   \n",
       "265         0.814732    0.797975          0.455194           0.230045   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst   \n",
       "204         0.213339              0.348797        0.285630  \\\n",
       "70          0.214617              0.614777        0.194362   \n",
       "131         0.302796              0.520275        0.250739   \n",
       "431         0.191933              0.253265        0.195348   \n",
       "540         0.143530              0.237732        0.150601   \n",
       "..               ...                   ...             ...   \n",
       "486         0.194649              0.269003        0.175439   \n",
       "75          0.225958              0.522337        0.213877   \n",
       "249         0.144089              0.330172        0.216637   \n",
       "238         0.244728              0.282440        0.064065   \n",
       "265         0.274920              0.570103        0.256850   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "204                 0.212908  \n",
       "70                  0.071166  \n",
       "131                 0.164961  \n",
       "431                 0.252853  \n",
       "540                 0.172504  \n",
       "..                       ...  \n",
       "486                 0.071625  \n",
       "75                  0.057917  \n",
       "249                 0.151187  \n",
       "238                 0.150335  \n",
       "265                 0.178014  \n",
       "\n",
       "[114 rows x 30 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Processed Data to Disk\n",
    "\n",
    "It can be useful to save processed data to disk, especially for really large datasets, to avoid repeating the preprocessing steps every time you start the Jupyter notebook. The parquet format is a fast and efficient format for saving and loading Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inputs: (341, 30)\n",
      "train_targets: (341,)\n",
      "val_inputs: (114, 30)\n",
      "val_targets: (114,)\n",
      "test_inputs: (114, 30)\n",
      "test_targets: (114,)\n"
     ]
    }
   ],
   "source": [
    "print('train_inputs:', train_inputs.shape)\n",
    "print('train_targets:', train_targets.shape)\n",
    "print('val_inputs:', val_inputs.shape)\n",
    "print('val_targets:', val_targets.shape)\n",
    "print('test_inputs:', test_inputs.shape)\n",
    "print('test_targets:', test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs.to_parquet('train_inputs.parquet')\n",
    "val_inputs.to_parquet('val_inputs.parquet')\n",
    "test_inputs.to_parquet('test_inputsMinMaxScaler.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.91 ms, sys: 6.21 ms, total: 12.1 ms\n",
      "Wall time: 10.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pd.DataFrame(train_targets).to_parquet('train_targets.parquet')\n",
    "pd.DataFrame(val_targets).to_parquet('val_targets.parquet')\n",
    "pd.DataFrame(test_targets).to_parquet('test_targets.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read the data back using `pd.read_parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.6 ms, sys: 1.23 ms, total: 34.8 ms\n",
      "Wall time: 22.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_inputs = pd.read_parquet('train_inputs.parquet')\n",
    "val_inputs = pd.read_parquet('val_inputs.parquet')\n",
    "test_inputs = pd.read_parquet('test_inputs.parquet')\n",
    "\n",
    "train_targets = pd.read_parquet('train_targets.parquet')[target_col]\n",
    "val_targets = pd.read_parquet('val_targets.parquet')[target_col]\n",
    "test_targets = pd.read_parquet('test_targets.parquet')[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inputs: (341, 30)\n",
      "train_targets: (341,)\n",
      "val_inputs: (114, 30)\n",
      "val_targets: (114,)\n",
      "test_inputs: (114, 30)\n",
      "test_targets: (114,)\n"
     ]
    }
   ],
   "source": [
    "print('train_inputs:', train_inputs.shape)\n",
    "print('train_targets:', train_targets.shape)\n",
    "print('val_inputs:', val_inputs.shape)\n",
    "print('val_targets:', val_targets.shape)\n",
    "print('test_inputs:', test_inputs.shape)\n",
    "print('test_targets:', test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95     M\n",
       "93     B\n",
       "401    B\n",
       "345    B\n",
       "194    M\n",
       "      ..\n",
       "212    M\n",
       "15     M\n",
       "428    B\n",
       "363    B\n",
       "116    B\n",
       "Name: diagnosis, Length: 114, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Logistic Regression (aka logit, MaxEnt) classifier.\n",
       "\n",
       "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
       "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
       "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
       "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
       "'sag', 'saga' and 'newton-cg' solvers.)\n",
       "\n",
       "This class implements regularized logistic regression using the\n",
       "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
       "that regularization is applied by default**. It can handle both dense\n",
       "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
       "floats for optimal performance; any other input format will be converted\n",
       "(and copied).\n",
       "\n",
       "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
       "with primal formulation, or no regularization. The 'liblinear' solver\n",
       "supports both L1 and L2 regularization, with a dual formulation only for\n",
       "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
       "'saga' solver.\n",
       "\n",
       "Read more in the :ref:`User Guide <logistic_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
       "    Specify the norm of the penalty:\n",
       "\n",
       "    - `None`: no penalty is added;\n",
       "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
       "    - `'l1'`: add a L1 penalty term;\n",
       "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
       "\n",
       "    .. warning::\n",
       "       Some penalties may not work with some solvers. See the parameter\n",
       "       `solver` below, to know the compatibility between the penalty and\n",
       "       solver.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
       "\n",
       "    .. deprecated:: 1.2\n",
       "       The 'none' option was deprecated in version 1.2, and will be removed\n",
       "       in 1.4. Use `None` instead.\n",
       "\n",
       "dual : bool, default=False\n",
       "    Dual or primal formulation. Dual formulation is only implemented for\n",
       "    l2 penalty with liblinear solver. Prefer dual=False when\n",
       "    n_samples > n_features.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for stopping criteria.\n",
       "\n",
       "C : float, default=1.0\n",
       "    Inverse of regularization strength; must be a positive float.\n",
       "    Like in support vector machines, smaller values specify stronger\n",
       "    regularization.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
       "    added to the decision function.\n",
       "\n",
       "intercept_scaling : float, default=1\n",
       "    Useful only when the solver 'liblinear' is used\n",
       "    and self.fit_intercept is set to True. In this case, x becomes\n",
       "    [x, self.intercept_scaling],\n",
       "    i.e. a \"synthetic\" feature with constant value equal to\n",
       "    intercept_scaling is appended to the instance vector.\n",
       "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
       "\n",
       "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
       "    as all other features.\n",
       "    To lessen the effect of regularization on synthetic feature weight\n",
       "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one.\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *class_weight='balanced'*\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
       "    data. See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
       "\n",
       "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
       "    To choose a solver, you might want to consider the following aspects:\n",
       "\n",
       "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
       "          and 'saga' are faster for large ones;\n",
       "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
       "          'lbfgs' handle multinomial loss;\n",
       "        - 'liblinear' is limited to one-versus-rest schemes.\n",
       "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
       "          especially with one-hot encoded categorical features with rare\n",
       "          categories. Note that it is limited to binary classification and the\n",
       "          one-versus-rest reduction for multiclass classification. Be aware that\n",
       "          the memory usage of this solver has a quadratic dependency on\n",
       "          `n_features` because it explicitly computes the Hessian matrix.\n",
       "\n",
       "    .. warning::\n",
       "       The choice of the algorithm depends on the penalty chosen.\n",
       "       Supported penalties by solver:\n",
       "\n",
       "       - 'lbfgs'           -   ['l2', None]\n",
       "       - 'liblinear'       -   ['l1', 'l2']\n",
       "       - 'newton-cg'       -   ['l2', None]\n",
       "       - 'newton-cholesky' -   ['l2', None]\n",
       "       - 'sag'             -   ['l2', None]\n",
       "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
       "\n",
       "    .. note::\n",
       "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
       "       with approximately the same scale. You can preprocess the data with\n",
       "       a scaler from :mod:`sklearn.preprocessing`.\n",
       "\n",
       "    .. seealso::\n",
       "       Refer to the User Guide for more information regarding\n",
       "       :class:`LogisticRegression` and more specifically the\n",
       "       :ref:`Table <Logistic_regression>`\n",
       "       summarizing solver/penalty supports.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       Stochastic Average Gradient descent solver.\n",
       "    .. versionadded:: 0.19\n",
       "       SAGA solver.\n",
       "    .. versionchanged:: 0.22\n",
       "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
       "    .. versionadded:: 1.2\n",
       "       newton-cholesky solver.\n",
       "\n",
       "max_iter : int, default=100\n",
       "    Maximum number of iterations taken for the solvers to converge.\n",
       "\n",
       "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
       "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
       "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
       "    across the entire probability distribution, *even when the data is\n",
       "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
       "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
       "    and otherwise selects 'multinomial'.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
       "    .. versionchanged:: 0.22\n",
       "        Default changed from 'ovr' to 'auto' in 0.22.\n",
       "\n",
       "verbose : int, default=0\n",
       "    For the liblinear and lbfgs solvers set verbose to any positive\n",
       "    number for verbosity.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    Number of CPU cores used when parallelizing over classes if\n",
       "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
       "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
       "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors.\n",
       "    See :term:`Glossary <n_jobs>` for more details.\n",
       "\n",
       "l1_ratio : float, default=None\n",
       "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
       "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
       "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
       "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
       "    combination of L1 and L2.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "\n",
       "classes_ : ndarray of shape (n_classes, )\n",
       "    A list of class labels known to the classifier.\n",
       "\n",
       "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
       "    Coefficient of the features in the decision function.\n",
       "\n",
       "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
       "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
       "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
       "\n",
       "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
       "    Intercept (a.k.a. bias) added to the decision function.\n",
       "\n",
       "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
       "    `intercept_` is of shape (1,) when the given problem is binary.\n",
       "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
       "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
       "    outcome 0 (False).\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
       "    Actual number of iterations for all classes. If binary or multinomial,\n",
       "    it returns only 1 element. For liblinear solver, only the maximum\n",
       "    number of iteration across all classes is given.\n",
       "\n",
       "    .. versionchanged:: 0.20\n",
       "\n",
       "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
       "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "SGDClassifier : Incrementally trained logistic regression (when given\n",
       "    the parameter ``loss=\"log\"``).\n",
       "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The underlying C implementation uses a random number generator to\n",
       "select features when fitting the model. It is thus not uncommon,\n",
       "to have slightly different results for the same input data. If\n",
       "that happens, try with a smaller tol parameter.\n",
       "\n",
       "Predict output may not match that of standalone liblinear in certain\n",
       "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
       "in the narrative documentation.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
       "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
       "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
       "\n",
       "LIBLINEAR -- A Library for Large Linear Classification\n",
       "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
       "\n",
       "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
       "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
       "    https://hal.inria.fr/hal-00860051/document\n",
       "\n",
       "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
       "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
       "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
       "\n",
       "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
       "    methods for logistic regression and maximum entropy models.\n",
       "    Machine Learning 85(1-2):41-75.\n",
       "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_iris\n",
       ">>> from sklearn.linear_model import LogisticRegression\n",
       ">>> X, y = load_iris(return_X_y=True)\n",
       ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
       ">>> clf.predict(X[:2, :])\n",
       "array([0, 0])\n",
       ">>> clf.predict_proba(X[:2, :])\n",
       "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
       "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
       ">>> clf.score(X, y)\n",
       "0.97...\n",
       "\u001b[0;31mFile:\u001b[0m           ~/products/conda/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     LogisticRegressionCV"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_inputs[numeric_cols], train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_inputs[numeric_cols]\n",
    "X_val = val_inputs[numeric_cols]\n",
    "X_test = test_inputs[numeric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B', 'M', 'M', 'B', 'B', 'B', 'B', 'B', 'M', 'B', 'B', 'M', 'B',\n",
       "       'B', 'B', 'M', 'B', 'B', 'B', 'M', 'B', 'B', 'B', 'M', 'B', 'B',\n",
       "       'B', 'B', 'M', 'B', 'M', 'B', 'B', 'B', 'B', 'B', 'B', 'M', 'B',\n",
       "       'B', 'B', 'M', 'M', 'B', 'M', 'M', 'B', 'B', 'M', 'B', 'M', 'B',\n",
       "       'M', 'M', 'M', 'M', 'B', 'B', 'M', 'B', 'B', 'B', 'B', 'B', 'M',\n",
       "       'M', 'B', 'M', 'B', 'M', 'B', 'B', 'B', 'B', 'B', 'M', 'M', 'B',\n",
       "       'B', 'M', 'B', 'M', 'M', 'B', 'B', 'M', 'B', 'M', 'B', 'B', 'B',\n",
       "       'B', 'M', 'B', 'B', 'M', 'B', 'B', 'B', 'B', 'B', 'B', 'M', 'B',\n",
       "       'B', 'M', 'M', 'M', 'B', 'M', 'B', 'M', 'B', 'M', 'M', 'B', 'M',\n",
       "       'B', 'B', 'B', 'B', 'M', 'M', 'M', 'B', 'B', 'B', 'M', 'M', 'M',\n",
       "       'B', 'M', 'M', 'M', 'B', 'B', 'M', 'B', 'B', 'B', 'M', 'B', 'B',\n",
       "       'B', 'M', 'B', 'M', 'M', 'M', 'B', 'B', 'M', 'M', 'B', 'M', 'B',\n",
       "       'M', 'M', 'M', 'B', 'B', 'B', 'B', 'M', 'B', 'B', 'M', 'M', 'B',\n",
       "       'M', 'B', 'M', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'M',\n",
       "       'B', 'B', 'M', 'M', 'M', 'B', 'B', 'B', 'B', 'B', 'B', 'M', 'M',\n",
       "       'B', 'B', 'B', 'B', 'B', 'M', 'B', 'M', 'B', 'B', 'B', 'B', 'B',\n",
       "       'M', 'M', 'M', 'M', 'B', 'B', 'B', 'M', 'M', 'B', 'B', 'M', 'B',\n",
       "       'B', 'M', 'B', 'M', 'M', 'B', 'B', 'B', 'M', 'B', 'B', 'B', 'B',\n",
       "       'M', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'M', 'B', 'B',\n",
       "       'B', 'B', 'M', 'B', 'B', 'M', 'B', 'B', 'B', 'B', 'B', 'B', 'B',\n",
       "       'B', 'B', 'B', 'B', 'M', 'B', 'M', 'M', 'B', 'B', 'M', 'B', 'B',\n",
       "       'B', 'B', 'M', 'M', 'B', 'M', 'B', 'B', 'B', 'M', 'B', 'B', 'B',\n",
       "       'M', 'M', 'B', 'M', 'B', 'B', 'B', 'B', 'M', 'B', 'B', 'B', 'B',\n",
       "       'M', 'B', 'B', 'M', 'B', 'B', 'B', 'B', 'B', 'M', 'M', 'B', 'M',\n",
       "       'B', 'B', 'B', 'M', 'B', 'M', 'M', 'B', 'B', 'B', 'B', 'M', 'B',\n",
       "       'B', 'B', 'B', 'B', 'B', 'B', 'M', 'B', 'B', 'B', 'M', 'M', 'M',\n",
       "       'B', 'M', 'B'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217    B\n",
       "283    M\n",
       "0      M\n",
       "504    B\n",
       "251    B\n",
       "      ..\n",
       "57     M\n",
       "492    M\n",
       "418    B\n",
       "385    M\n",
       "325    B\n",
       "Name: diagnosis, Length: 341, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736070381231672"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_targets, train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves an accuracy of 97.3% on the training set. We can visualize the breakdown of correctly and incorrectly classified inputs using a confusion matrix.\n",
    "\n",
    "<img src=\"https://i.imgur.com/UM28BCN.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99537037, 0.00462963],\n",
       "       [0.064     , 0.936     ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train_targets, train_preds, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(inputs, targets, name=''):\n",
    "    preds = model.predict(inputs)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, preds)\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "    \n",
    "    cf = confusion_matrix(targets, preds, normalize='true')\n",
    "    plt.figure()\n",
    "    sns.heatmap(cf, annot=True)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Target')\n",
    "    plt.title('{} Confusion Matrix'.format(name));\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.36%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_preds = predict_and_plot(X_train, train_targets, 'Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.61%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_preds = predict_and_plot(X_val, val_targets, 'Validatiaon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.49%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAHHCAYAAAAMD3r6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6J0lEQVR4nO3deVxV1f7/8fcBGQQUUQZnTc0pUwuVNM0sErVMLMtvllNlZmYmWUk3xTG6lUZeTcqcU7McyulqRlk55VxWzkOWxeSYmExn//7o57kdQWXYG4TzevbYjxvrrL3W2odLfPh81t7HZhiGIQAAAJO4FfcCAABA6UJwAQAATEVwAQAATEVwAQAATEVwAQAATEVwAQAATEVwAQAATEVwAQAATEVwAQAATEVwAZQQSUlJ6tGjhypVqiSbzaa4uDjT57DZbBo9erTp45ZU/fr1U+3atYt7GUCJQ3CB64bNZsvTsX79+kLPdeHCBY0ePTrfYyUlJWn48OFq2LChfHx85Ovrq9DQUI0fP15nzpwp9LquZtiwYVq7dq2io6M1b948derUydL5itLo0aNls9nk5uamX3/9Ncfr586dU9myZWWz2fTss8/me/yCfr8BFEyZ4l4AcMm8efOcvp47d67WrVuXo71Ro0aFnuvChQsaM2aMJOnOO+/M0znbtm1Tly5ddP78eT322GMKDQ2VJG3fvl2vv/66vvnmG33++eeFXtuVfPnll+rWrZuGDx9u2Rx//fWXypQpvv8seHl5aeHChXrppZec2pcuXVqocQvy/Zak6dOny263F2puwBURXOC68dhjjzl9vWXLFq1bty5He3E4c+aMunfvLnd3d+3atUsNGzZ0en3ChAmaPn26pWtITk5WhQoVLJ3D29vb0vGvpUuXLrkGFwsWLNC9996rJUuWFMk60tLS5OvrKw8PjyKZDyhtKIugRLHb7YqLi9NNN90kb29vhYSEaODAgTp9+rRTv+3btysiIkKBgYEqW7asbrjhBj3++OOSpGPHjikoKEiSNGbMGEe55Wp7Dd577z2dOHFCkyZNyhFYSFJISIheffVVp7Z3331XN910k7y8vFS1alUNHjw4R+nkzjvvVJMmTfTzzz+rQ4cO8vHxUbVq1fTGG284+syePVs2m02GYWjq1KmO9Ur/Kydc7tI5x44dy9N7cklu78OuXbvUuXNnlS9fXn5+frr77ru1ZcuWXOfbuHGjoqKiFBQUJF9fX3Xv3l0pKSlXfF8v16tXL+3evVv79u1ztCUmJurLL79Ur169cvTPyMjQqFGjFBoaKn9/f/n6+qpdu3b66quvHH2u9f3u16+f/Pz8dPjwYXXp0kXlypXTo48+6njtn3suYmJi5ObmpoSEBKd1PPXUU/L09NT333+f52sFSjMyFyhRBg4cqNmzZ6t///567rnndPToUU2ZMkW7du3Sxo0b5eHhoeTkZHXs2FFBQUEaMWKEKlSooGPHjjlS60FBQZo2bZoGDRqk7t2764EHHpAkNW3a9IrzLl++XGXLllWPHj3ytM7Ro0drzJgxCg8P16BBg7R//35NmzZN27Ztc6zzktOnT6tTp0564IEH9PDDD2vx4sV6+eWXdfPNN6tz58664447NG/ePPXu3Vv33HOP+vTpk+/37VrvyZX89NNPateuncqXL6+XXnpJHh4eeu+993TnnXfq66+/VlhYmFP/IUOGKCAgQDExMTp27Jji4uL07LPPatGiRXla5x133KHq1atrwYIFGjt2rCRp0aJF8vPz07333puj/7lz5/TBBx/okUce0YABA/Tnn39qxowZioiI0NatW9W8efM8fb+zsrIUERGhtm3b6q233pKPj0+u63v11Ve1YsUKPfHEE9qzZ4/KlSuntWvXavr06Ro3bpyaNWuWp+sESj0DuE4NHjzY+Of/Rb/99ltDkjF//nynfmvWrHFqX7ZsmSHJ2LZt2xXHTklJMSQZMTExeVpLQECA0axZszz1TU5ONjw9PY2OHTsa2dnZjvYpU6YYkoyZM2c62tq3b29IMubOnetoS09PNypXrmw8+OCDTuNKMgYPHuzUFhMTY+T2Yzxr1ixDknH06FHDMPL2nlya45/vSWRkpOHp6WkcPnzY0fb7778b5cqVM+64444c84WHhxt2u93RPmzYMMPd3d04c+bMVee9dB0pKSnG8OHDjXr16jlea9mypdG/f/9c34OsrCwjPT3daazTp08bISEhxuOPP+5ou9r3u2/fvoYkY8SIEbm+VqtWLae2PXv2GJ6ensaTTz5pnD592qhWrZrRokULIzMz86rXCLgSyiIoMT755BP5+/vrnnvuUWpqquMIDQ2Vn5+fIxV+aV/CypUrlZmZacrc586dU7ly5fLU94svvlBGRoaef/55ubn970dswIABKl++vFatWuXU38/Pz2lfiaenp1q1aqUjR46YsnapYO9Jdna2Pv/8c0VGRqpOnTqO9ipVqqhXr17asGGDzp0753TOU0895VSmadeunbKzs/XLL7/kea29evXSoUOHtG3bNsf/5lYSkSR3d3d5enpK+rtkdurUKWVlZalFixbauXNnnueUpEGDBuWpX5MmTTRmzBh98MEHioiIUGpqqubMmVOsG2GB6w3BBUqMgwcP6uzZswoODlZQUJDTcf78eSUnJ0uS2rdvrwcffFBjxoxRYGCgunXrplmzZik9Pb3Ac5cvX15//vlnnvpe+kXaoEEDp3ZPT0/VqVMnxy/a6tWr59g3ERAQkGMfSWEU5D1JSUnRhQsXclyH9PcdO3a7PcdtozVr1nT6OiAgQJLydS233HKLGjZsqAULFmj+/PmqXLmy7rrrriv2nzNnjpo2bSpvb29VqlRJQUFBWrVqlc6ePZvnOcuUKaPq1avnuf+LL76oZs2aaevWrYqJiVHjxo3zfC7gCgi1UWLY7XYFBwdr/vz5ub5+adOezWbT4sWLtWXLFq1YsUJr167V448/rokTJ2rLli3y8/PL99wNGzbU7t27lZGR4fhL2Szu7u65thuGcc1zc9vMKf2ddbi8n9nvSW4Kcy3/1KtXL02bNk3lypVTz549nTJA//Thhx+qX79+ioyM1Isvvqjg4GC5u7srNjZWhw8fzvN8Xl5eV5wjN0eOHNHBgwclSXv27MnzeYCrIHOBEqNu3bo6efKkbr/9doWHh+c4Lt9Md9ttt2nChAnavn275s+fr59++kkfffSRpCv/Ur6Srl276q+//srTrZC1atWSJO3fv9+pPSMjQ0ePHnW8boZLmYHL70K5Uhniau/J5YKCguTj45PjOiRp3759cnNzU40aNQp3AVfQq1cv/fHHHzpw4MAVSyKStHjxYtWpU0dLly5V7969FRERofDwcF28eNGpX36/31djt9vVr18/lS9fXq+88ooWLlxY6OdwAKUNwQVKjIcffljZ2dkaN25cjteysrIcv2BPnz6d4y/l5s2bS5KjDHDpboC8PlXz6aefVpUqVfTCCy/owIEDOV5PTk7W+PHjJUnh4eHy9PTU5MmTndYxY8YMnT17Nte7Hgqqbt26kqRvvvnG0ZaWlqY5c+Y49cvLe3I5d3d3dezYUZ999pnTLa1JSUlasGCB2rZtq/Lly5twFTnVrVtXcXFxio2NVatWra7Y71Km5J/X9t1332nz5s1O/fL7/b6aSZMmadOmTXr//fc1btw4tWnTRoMGDVJqamqhxwZKC8oiKDHat2+vgQMHKjY2Vrt371bHjh3l4eGhgwcP6pNPPtE777yjHj16aM6cOXr33XfVvXt31a1bV3/++aemT5+u8uXLq0uXLpKksmXLqnHjxlq0aJHq16+vihUrqkmTJmrSpEmucwcEBGjZsmXq0qWLmjdv7vSEzp07d2rhwoVq3bq1pL//4o+OjtaYMWPUqVMn3X///dq/f7/effddtWzZ0tSHgnXs2FE1a9bUE088oRdffFHu7u6aOXOmgoKCdPz4cUe/vLwnuRk/frzWrVuntm3b6plnnlGZMmX03nvvKT093elZHFYYOnToNfvcd999Wrp0qbp37657771XR48eVXx8vBo3bqzz5887+uX3+30le/fu1ciRI9WvXz917dpV0t/P+GjevLmeeeYZffzxx/m7SKC0Ks5bVYCrufxW1Evef/99IzQ01ChbtqxRrlw54+abbzZeeukl4/fffzcMwzB27txpPPLII0bNmjUNLy8vIzg42LjvvvuM7du3O42zadMmIzQ01PD09Mzzbam///67MWzYMKN+/fqGt7e34ePjY4SGhhoTJkwwzp4969R3ypQpRsOGDQ0PDw8jJCTEGDRokHH69GmnPu3btzduuummHPPkdgukcrkV1TAMY8eOHUZYWJjh6elp1KxZ05g0aVKOW1Hz+p7k9j7s3LnTiIiIMPz8/AwfHx+jQ4cOxqZNm5z6XJrv8ltdv/rqK0OS8dVXX+VY9z/981bUq7n8PbDb7cZrr71m1KpVy/Dy8jJuueUWY+XKlbm+f1f6fvft29fw9fXNdb5/jpOVlWW0bNnSqF69eo5ba9955x1DkrFo0aKrrh9wFTbDyOdOKwAAgKtgzwUAADAVwQUAADAVwQUAADAVwQUAADAVwQUAADAVwQUAADAVwQUAADBVqXxCZ2aqeR9VDZQmZau2K+4lANedrIwTls9h1u8lj8A6poxjNTIXAADAVKUycwEAwHXFnl3cKyhSBBcAAFjNsBf3CooUwQUAAFazu1ZwwZ4LAABgKjIXAABYzKAsAgAATEVZBAAAoODIXAAAYDXKIgAAwFQu9pwLyiIAAMBUZC4AALAaZREAAGAq7hYBAAAoODIXAABYjIdoAQAAc7lYWYTgAgAAq7lY5oI9FwAAwFRkLgAAsJqLPUSL4AIAAKtRFgEAACg4MhcAAFiNu0UAAICpKIsAAAAUHJkLAACsRlkEAACYyTBc61ZUyiIAAMBUZC4AALCai23oJLgAAMBq7LkAAACmcrHMBXsuAACAqchcAABgNT64DAAAmIqyCAAAQMGRuQAAwGrcLQIAAExFWQQAAKDgyFwAAGA1yiIAAMBULhZcUBYBAACmInMBAIDFXO0j1wkuAACwmouVRQguAACwGreiAgAAFByZCwAArEZZBAAAmIqyCAAAQMGRuQAAwGqURQAAgKkoiwAAABQcmQsAAKxGWQQAAJjKxYILyiIAAMBUZC4AALCai23oJLgAAMBqLlYWIbgAAMBqLpa5YM8FAAAwFZkLAACsRlkEAACYirIIAABAwZG5AADAapRFAACAqVwsuKAsAgAATEXmAgAAqxlGca+gSBFcAABgNcoiAAAABUdwAQCA1ex2c44CmDp1qmrXri1vb2+FhYVp69atV+0fFxenBg0aqGzZsqpRo4aGDRumixcv5mtOyiIAAFitmB6itWjRIkVFRSk+Pl5hYWGKi4tTRESE9u/fr+Dg4Bz9FyxYoBEjRmjmzJlq06aNDhw4oH79+slms2nSpEl5npfMBQAAViumzMWkSZM0YMAA9e/fX40bN1Z8fLx8fHw0c+bMXPtv2rRJt99+u3r16qXatWurY8eOeuSRR66Z7bgcwQUAACVEenq6zp0753Skp6fn2jcjI0M7duxQeHi4o83NzU3h4eHavHlzrue0adNGO3bscAQTR44c0erVq9WlS5d8rZPgAgAAqxmGKUdsbKz8/f2djtjY2FynTE1NVXZ2tkJCQpzaQ0JClJiYmOs5vXr10tixY9W2bVt5eHiobt26uvPOO/XKK6/k63IJLgAAsJpJZZHo6GidPXvW6YiOjjZtmevXr9drr72md999Vzt37tTSpUu1atUqjRs3Ll/jsKETAIASwsvLS15eXnnqGxgYKHd3dyUlJTm1JyUlqXLlyrmeM3LkSPXu3VtPPvmkJOnmm29WWlqannrqKf3rX/+Sm1vechJkLgAAsFoxbOj09PRUaGioEhIS/rEMuxISEtS6detcz7lw4UKOAMLd3V2SZOTjKaNkLgAAsFox3YoaFRWlvn37qkWLFmrVqpXi4uKUlpam/v37S5L69OmjatWqOfZtdO3aVZMmTdItt9yisLAwHTp0SCNHjlTXrl0dQUZeEFwAAFBK9ezZUykpKRo1apQSExPVvHlzrVmzxrHJ8/jx406ZildffVU2m02vvvqqTpw4oaCgIHXt2lUTJkzI17w2Iz95jhIiM/VIcS8BuC6VrdquuJcAXHeyMk5YPseF94eZMo7PU2+bMo7VyFwAAGA1PrgMAACg4MhcAABgtWLa0FlcCC4AALCavdRtb7wqggsAAKzGngsAAICCI3MBAIDVXCxzQXABAIDVSt8jpa6KsggAADAVwQXyZeGSFer4YF/d2uF+PTLgee35ef8V+2ZmZWnazPnq9FB/3drhfj3Q9xlt2LLdqU9a2gW9Hhevex7oq9AO3fTowCjt2XvlMYHrwaCn++rQgS06f+6wNm1YoZYtml+1/4MP3qcf93yt8+cOa9fOL9S5011Or0dGdtZ/Vy1Q0h8/KivjhJo1u+mq461cPk9ZGSd0//0Rhb0UFJVi+OCy4kRwgTz77xdf643/vK9Bjz+qT2b+Rw3q3aCBUa/q5Okzufb/z/tz9Mln/9Urwwbpsw/f08ORXTQ0epz2Hjjk6DPq9Xe0edsuxY4armXzpqlNq1s1YOgrSkpJLaKrAvLnoYfu11tvxmjc+ElqGdZJ3//ws1avmq+goEq59m99WwvNnzdVs2YtVItWEVq+fK2WLJ6hm25q4Ojj6+ujjZu2KvqVa39+w9DnBuTr0ylxnbAb5hwlBMEF8mzuomXq0bWzut/bUXVvqKVRLw6Rt5eXlq38PNf+K9Z8qQF9euqONq1Uo1oV/V/3+9SudUvNXrhUknQxPV1ffL1BUYOfUIvmN6tm9aoa/MRjqlm9qhYtW1WUlwbk2bChA/TBjAWaM/dj7d17UM8MHqELF/5S/37/l2v/IUOe0Nq16zVxUrz27TukmNFvateuH/XMoP6OPvPnL9H4CXFK+PLbq87drNlNGvb8QD351AumXhNgtmLd0JmamqqZM2dq8+bNSkxMlCRVrlxZbdq0Ub9+/RQUFFScy8M/ZGZm6uf9B/Vk74cdbW5ubrqtRXN9/+PeXM/JyMyUp6enU5uXl6d2/fCTJCk7K1vZ2XZ5eXrk6LPz//cBriceHh669damev2NKY42wzCU8OUG3XZbaK7n3BYWqrh33ndq+3zdet1/f6d8zV22rLfmzZ2iIUNfUVJSSv4Xj+LlYk/oLLbMxbZt21S/fn1NnjxZ/v7+uuOOO3THHXfI399fkydPVsOGDbV9+/ZrD4QicfrMOWVn21WpYoBTe6WKAUo9dTrXc24PC9Xcj5bql19PyG63a9PWnUr4epNSTp6S9HcquFmTRoqfvVDJKSeVnZ2tFWu/1Pc/7lNq6inLrwnIr8DAiipTpoySk5zLdsnJKaockvsfQ5UrBykp2TkYSEpKvWL/K5n41hht3rxdK1bkninEdc7FyiLFlrkYMmSIHnroIcXHx8tmszm9ZhiGnn76aQ0ZMkSbN2++6jjp6elKT093anNLT5eXl5fpa0b+jBg6UKP/PVldez0lm02qUbWKIu+9x6mMEjtyuEbFvq27Ih+Tu7ubGtWvp87h7fXz/kNXGRlwLffdd4863Hm7WrTqWNxLAfKk2IKL77//XrNnz84RWEiSzWbTsGHDdMstt1xznNjYWI0ZM8ap7dUXn9Ool4aatlZIARXKy93dTScvy1KcPHVagZdlMy6pGFBBk18fpfT0DJ05d07BgZX09rSZql61sqNPzepVNXvqm7rw10WlpV1QUGBFvTAy1qkPcL1ITT2lrKwsBYcEOrUHBwcp8QqlisTEFIUEO2cpQkICr9g/Nx3ubKu6dWvpZIpzCfKTRdO1YcN3uvueh/I8FoqHUYLu9DBDsZVFKleurK1bt17x9a1btyokJOSa40RHR+vs2bNOx8tDnzZzqdDftebGDW7Ud9t3O9rsdru+27FbzZo0uuq5Xl6eCgkKVFZ2ttat36gO7Vrn6ONT1ltBgRV19tyf2rR1h+5qd5vZlwAUWmZmpnbu/EF3dWjraLPZbLqrQ1tt2bIj13O2fLdDd93V1qkt/O47rtg/N2+8OUW3hIYrtGVHxyFJLwwfrScGRBXgSlDkKIsUjeHDh+upp57Sjh07dPfddzsCiaSkJCUkJGj69Ol66623rjmOl5dXjhJIZga3MVqhT8/u+teEibqp4Y1q0riBPvz4U/11MV2R994jSYoe95aCAytp2P/fBf/DT/uUlHJSDW+so+SUk3p35ocyDEOPP9rDMebG73bIMAzVrlldx3/7XROnztANNasr8l7Sv7g+vf3OdM2a8bZ27PxB27bt0nNDBsjXt6xmz1kkSZo18x39/vsf+terr0uS/vOfGfoyYbGGPT9Qq//7hXo+3E2hoU319DMvOcYMCKigmjWrqWqVv/87WL9+XUlSYmKykpJSHMfljv96QseO/Wr1JcMMLrahs9iCi8GDByswMFBvv/223n33XWVnZ0uS3N3dFRoaqtmzZ+vhhx++xigoSp3D2+v0mbOa8sGHSj11Sg1vrKv4ieMcZZE/kpLl9o8yV3pGhv4zfY5++z1RPmXLql3rlood+aLKl/Nz9PnzfJri4mcpKSVV/uXL6Z72bfXcwL7yKMOT6XF9+uST5QoKrKjRo4arcuUgff/9T7r3vseUnPz3HzU1a1SV/R8p8M1btuuxPs9q7JiXNH7cyzp46Kge7PGEfvrpfw+L63pfR82c8bbj64Xzp0mSxo6bqLHjJhXRlQHmsRnXwdNYMjMzlZr69w9mYGCgPDw8rnHGNcZLPWLGsoBSp2zVdsW9BOC6k5VxwvI50sY+aso4vqPmmzKO1a6LPw89PDxUpUqV4l4GAADWYEMnAABAwV0XmQsAAEq1EnSnhxkILgAAsJqL3S1CWQQAAJiKzAUAAFajLAIAAMzE478BAAAKgcwFAABWoywCAABMRXABAABMxa2oAAAABUfmAgAAq1EWAQAAZjJcLLigLAIAAExF5gIAAKu5WOaC4AIAAKvxhE4AAICCI3MBAIDVKIsAAABTuVhwQVkEAACYiswFAAAWMwzXylwQXAAAYDUXK4sQXAAAYDUXCy7YcwEAAExF5gIAAIu52meLEFwAAGA1FwsuKIsAAABTkbkAAMBqrvXRIgQXAABYzdX2XFAWAQAApiJzAQCA1Vwsc0FwAQCA1VxszwVlEQAAYCoyFwAAWMzVNnQSXAAAYDUXK4sQXAAAYDFXy1yw5wIAAJiKzAUAAFajLAIAAMxkuFhwQVkEAACYiswFAABWc7HMBcEFAAAWoywCAABQCGQuAACwGpkLAABgJsNuzlEQU6dOVe3ateXt7a2wsDBt3br1qv3PnDmjwYMHq0qVKvLy8lL9+vW1evXqfM1J5gIAAIsV156LRYsWKSoqSvHx8QoLC1NcXJwiIiK0f/9+BQcH5+ifkZGhe+65R8HBwVq8eLGqVaumX375RRUqVMjXvDbDMErdM0kzU48U9xKA61LZqu2KewnAdScr44TlcyTf3d6UcYITvs5X/7CwMLVs2VJTpkyRJNntdtWoUUNDhgzRiBEjcvSPj4/Xm2++qX379snDw6PA66QsAgCAxcwqi6Snp+vcuXNOR3p6eq5zZmRkaMeOHQoPD3e0ubm5KTw8XJs3b871nOXLl6t169YaPHiwQkJC1KRJE7322mvKzs7O1/USXAAAYDXDZsoRGxsrf39/pyM2NjbXKVNTU5Wdna2QkBCn9pCQECUmJuZ6zpEjR7R48WJlZ2dr9erVGjlypCZOnKjx48fn63LZcwEAQAkRHR2tqKgopzYvLy/Txrfb7QoODtb7778vd3d3hYaG6sSJE3rzzTcVExOT53EILgAAsJhZGzq9vLzyHEwEBgbK3d1dSUlJTu1JSUmqXLlyrudUqVJFHh4ecnd3d7Q1atRIiYmJysjIkKenZ57mpiwCAIDFDLvNlCM/PD09FRoaqoSEBEeb3W5XQkKCWrdunes5t99+uw4dOiS7/X/R0IEDB1SlSpU8BxYSwQUAAKVWVFSUpk+frjlz5mjv3r0aNGiQ0tLS1L9/f0lSnz59FB0d7eg/aNAgnTp1SkOHDtWBAwe0atUqvfbaaxo8eHC+5qUsAgCAxYrrORc9e/ZUSkqKRo0apcTERDVv3lxr1qxxbPI8fvy43Nz+l2eoUaOG1q5dq2HDhqlp06aqVq2ahg4dqpdffjlf8/KcC8CF8JwLIKeieM7FidZ3mTJOtc1fmjKO1SiLAAAAU1EWAQDAYq72kesEFwAAWCy/d3qUdAQXAABYrPTtbrw69lwAAABTkbkAAMBilEUAAICpXC24oCwCAABMReYCAACLudqGToILAAAsRlkEAACgEPIdXNSpU0cnT57M0X7mzBnVqVPHlEUBAFCaGIbNlKOkyHdZ5NixY8rOzs7Rnp6erhMnrP/wFwAAShoe/30Fy5cvd/z72rVr5e/v7/g6OztbCQkJql27tqmLAwAAJU+eg4vIyEhJks1mU9++fZ1e8/DwUO3atTVx4kRTFwcAQGlgL0ElDTPkObiw2//O6dxwww3atm2bAgMDLVsUAAClSUnaL2GGfO+5OHr0qOPfL168KG9vb1MXBABAacOtqNdgt9s1btw4VatWTX5+fjpy5IgkaeTIkZoxY4bpCwQAACVLvoOL8ePHa/bs2XrjjTfk6enpaG/SpIk++OADUxcHAEBpYBjmHCVFvoOLuXPn6v3339ejjz4qd3d3R3uzZs20b98+UxcHAEBpYNhtphwlRb6DixMnTqhevXo52u12uzIzM01ZFAAAKLnyHVw0btxY3377bY72xYsX65ZbbjFlUQAAlCZ2w2bKUVLk+26RUaNGqW/fvjpx4oTsdruWLl2q/fv3a+7cuVq5cqUVawQAoERztVtR85256Natm1asWKEvvvhCvr6+GjVqlPbu3asVK1bonnvusWKNAACgBCnQR663a9dO69atM3stAACUSiXpTg8zFCi4AAAAeVeS9kuYId/BRUBAgGy2nG+SzWaTt7e36tWrp379+ql///6mLBAAAJQsBdrQOWHCBHXu3FmtWrWSJG3dulVr1qzR4MGDdfToUQ0aNEhZWVkaMGCA6QsGAKCkcbUNnfkOLjZs2KDx48fr6aefdmp/77339Pnnn2vJkiVq2rSpJk+eTHABAIBcb89Fvu8WWbt2rcLDw3O033333Vq7dq0kqUuXLo7PHAEAwNW52nMu8h1cVKxYUStWrMjRvmLFClWsWFGSlJaWpnLlyhV+dQAAoMTJd1lk5MiRGjRokL766ivHnott27Zp9erVio+PlyStW7dO7du3N3el+VC9bpdimxu4nqV9/2FxLwFwSey5uIYBAwaocePGmjJlipYuXSpJatCggb7++mu1adNGkvTCCy+Yu0oAAEqwklTSMEO+govMzEwNHDhQI0eO1MKFC61aEwAAKMHytefCw8NDS5YssWotAACUSoZJR0mR7w2dkZGR+vTTTy1YCgAApZOr3S2S7z0XN954o8aOHauNGzcqNDRUvr6+Tq8/99xzpi0OAACUPDbDyN+jPW644YYrD2azXRfPtwjxb1jcSwCuS8e3TCvuJQDXHa9GHSyfY2PlHqaMc3viYlPGsVq+MxdHjx61Yh0AAJRa9uJeQBHL954LAACAqynQR67/9ttvWr58uY4fP66MjAyn1yZNmmTKwgAAKC0MlZzNmGbId3CRkJCg+++/X3Xq1NG+ffvUpEkTHTt2TIZh6NZbb7VijQAAlGj2knQfqQnyXRaJjo7W8OHDtWfPHnl7e2vJkiX69ddf1b59ez300ENWrBEAgBLNLpspR0mR7+Bi79696tOnjySpTJky+uuvv+Tn56exY8fq3//+t+kLBAAAJUu+gwtfX1/HPosqVaro8OHDjtdSU1PNWxkAAKWEIZspR0mR5+Bi7NixSktL02233aYNGzZIkrp06aIXXnhBEyZM0OOPP67bbrvNsoUCAFBS2U06Soo8P0TL3d1df/zxh86fP6/z58+radOmSktL0wsvvKBNmzbpxhtv1KRJk1SrVi2r13xNPEQLyB0P0QJyKoqHaK0L6WnKOPckLTJlHKvl+W6RSzFInTp1HG2+vr6Kj483f1UAAJQiJamkYYZ83Ypqs7nWmwMAgBlKUknDDPkKLurXr3/NAOPUqVOFWhAAACjZ8hVcjBkzRv7+/latBQCAUonMxVX83//9n4KDg61aCwAApZKr7bnI862o7LcAAAB5ke+7RQAAQP7YXezv8zwHF3a7q1WMAAAwR0n6XBAzFOgj1wEAQN65Wu4/358tAgAAcDVkLgAAsJirbSwguAAAwGJ2F7vjkrIIAAAwFZkLAAAs5mobOgkuAACwmKvtuaAsAgAATEXmAgAAi/GETgAAYCpXe0InZREAAEqxqVOnqnbt2vL29lZYWJi2bt2ap/M++ugj2Ww2RUZG5ntOggsAACxmmHTk16JFixQVFaWYmBjt3LlTzZo1U0REhJKTk6963rFjxzR8+HC1a9euALMSXAAAYDm7zZwjvyZNmqQBAwaof//+aty4seLj4+Xj46OZM2de8Zzs7Gw9+uijGjNmjOrUqVOg6yW4AADAYnaTjvT0dJ07d87pSE9Pz3XOjIwM7dixQ+Hh4Y42Nzc3hYeHa/PmzVdc69ixYxUcHKwnnniiwNdLcAEAQAkRGxsrf39/pyM2NjbXvqmpqcrOzlZISIhTe0hIiBITE3M9Z8OGDZoxY4amT59eqHVytwgAABYz6wmd0dHRioqKcmrz8vIyZew///xTvXv31vTp0xUYGFiosQguAACwmFnPufDy8spzMBEYGCh3d3clJSU5tSclJaly5co5+h8+fFjHjh1T165dHW12+9/PFi1Tpoz279+vunXr5mluyiIAAJRCnp6eCg0NVUJCgqPNbrcrISFBrVu3ztG/YcOG2rNnj3bv3u047r//fnXo0EG7d+9WjRo18jw3mQsAACxWXJ8tEhUVpb59+6pFixZq1aqV4uLilJaWpv79+0uS+vTpo2rVqik2Nlbe3t5q0qSJ0/kVKlSQpBzt10JwAQCAxYoruOjZs6dSUlI0atQoJSYmqnnz5lqzZo1jk+fx48fl5mZ+EcNmGEap+yTYEP+Gxb0E4Lp0fMu04l4CcN3xatTB8jneq/6YKeMM/O1DU8axGpkLAAAsZrjWR4sQXAAAYLXiKosUF+4WAQAApiJzAQCAxVwtc0FwAQCAxUrdnRPXQHABAIDFzHpCZ0nBngsAAGAqMhcAAFiMPRcAAMBUrhZcUBYBAACmInMBAIDFuFsEAACYirtFAAAACoHMBQAAFnO1DZ0EFwAAWMzV9lxQFgEAAKYicwEAgMXsLpa7ILgAAMBi7LkAAACmcq28BXsuAACAychcAABgMcoiAADAVDyhEwAAoBDIXAAAYDFuRQUAAKZyrdCCsggAADAZmQsAACzG3SIAAMBUrrbngrIIAAAwFZkLAAAs5lp5C4ILAAAsx54LAABgKvZcAAAAFAKZCwAALOZaeQuCCwAALOdqey4oiwAAAFORuQAAwGKGixVGCC4AALAYZREAAIBCIHMBAIDFXO05FwQXAABYzLVCC8oiAADAZGQukC/9n+ylZ557QsEhgfr5x3165cXx2rVzzxX7d42M0Mv/GqoaNavp6OFfNC7mLSWs+8bxetLZfbmeN2bkG3p38kzT1w9Y4aPV6zV72edKPXNO9WtXV/SAnrq5/g259s3MytaMJWu0/MvNSj51RrWrhej5Pg+o7a03Ofos+u/X+njNN/o9+aQkqW7NKhr48L1qF9qkSK4H5nO1sgiZC+RZtwc6a8xrIzTx31N1zx0P6Kcf9+ujZR8oMLBirv1btLpF8TMmasG8xQpv113/XfWFZi+YooaNbnT0aXJjW6dj6DOvyG63a9Xyz4vqsoBCWbNhu96cuVhP/999WjTpFTWoXV1Pj/mPTp45l2v/KfM/0+K13yh6QE99+p8YPRRxh4a9Hq+9R447+oRUCtDzvSP10cRoLXwrWq1ubqChsdN06PjvRXVZMJndpKOkILhAnj09uJ8+nPOJPpq/VAf2H9aLz8forwsX9UjvB3Pt/9Sg3vrqiw16d/JMHTxwRP+eMFl7vv9Zjz/1qKNPSnKq09Gpy13a+O13+uXYb0V1WUChzP3sCz3Y8XZF3t1GdWtU1chBvVTWy0OfJmzKtf/K9d/pyR6d1a7FzapeOUg9O7dX21ubaO5nXzj63Nmqqdq1uFm1qoaodrUQPfdYpHy8vfTD/qNFdVkwmWHSPyUFwQXyxMPDQ02b36Rv1//vP5iGYeib9ZvVomXzXM8Jbdlc36x3/g/sVwkbr9g/KKiSwiPaa8HcJWYtG7BUZmaW9h4+rtuaNnK0ubm5KaxZI32//0iu52RkZcnTw8OpzdvLQ7t+PpRr/+xsu/777Tb9dTFDzRrmXmoBrjfX9Z6LX3/9VTExMZo588q19/T0dKWnpzu1GYZdNhtxk5kqVgpQmTJllPL/a8CXpKSk6sYr1JaDQwJz7R8cEphr/4d7Rer8+TStWkFJBCXD6T/PK9tuV6UK5Z3aK/mX09HfEnM9p03zxpq3/AuF3lRPNSoH6bsf9ilh8y5l253/Kj1w7IR6j3hDGRmZ8vH2UtyIgapbo6pl1wJrlaSShhmu69/Ap06d0pw5c67aJzY2Vv7+/k5HWvqpIlohzPTIYw9q6ccrlZ6eUdxLASzz8pMPq2aVYHV7drRCezyr195fpG53t5Gbm82p3w3VQvTJ2//S/Dde1sOd79Crk+fo8K/suSipXK0sUqyZi+XLl1/19SNHck8r/lN0dLSioqKc2upVb1GodSGnUydPKysrS0HBlZzag4IClZyUmus5yUmpee4f1jpUN9avo6f6DzNv0YDFAsr5yd3NLcfmzZNn/1RgQPlcz6noX07vvDJI6RmZOvPneQVXrKC4uctU/bKMnodHGdWsEixJalyvln48+Ivmr/hKo555NLdhgetKsQYXkZGRstlsMowrR2M2m+2Kr0mSl5eXvLy8Ljvnuk7IlEiZmZn6YfdPate+tf67KkHS39+bdu1v08zp83M9Z8e23WrXvrXenzbX0da+Qxtt37Y7R99evXto964f9fOP+y1ZP2AFD48yalS3pr77YZ/uuq25JMlut+u7H/bpkS53XvVcL08PhVQKUGZWtr7YvEsdbw+9an+7YSgjM9OklaOoURYpQlWqVNHSpUtlt9tzPXbu3Fmcy8Nl4qfO1qN9H9LDj0Tqxvp19Mbbo+XjW1YffbhUkvSf+Nf1r5j/ZZHenzZPHcLb6uln+6vejTdo+Ihn1eyWmzTzfedgxK+cr+6PjND8uZ8U6fUAZujTLVxL1m3QZ19u1pFf/9D4+IX662KGIu9uI0l6JW6W3pm3zNH/hwNH9cXmXfotMUU7fjqoQWMmy24Y6t+9o6PPO/OWaftPB3UiKVUHjp34++sfD+je9q2K/PpgDrthmHKUFMWauQgNDdWOHTvUrVu3XF+/VlYDReuzpf9VpUoV9dIrQxQcEqSf9uzVIw8MUErK35s2q1WvKvs/NqVt37pLg54crhGvPq9XRg3T0cPH1K/Xs9q396DTuN0fvFey2bRs8aoivR7ADJ3attDps3/q3YUrlHr6nBrcUF3TYoY4NnkmppyS2z8ysBkZmZoy/zP9lpQqH28vtQ1toteG9Vd5Px9Hn1Nn/tSrcbOUcvqc/HzLqn6taoqPGaLWzRsX+fUBBWEzivG397fffqu0tDR16tQp19fT0tK0fft2tW/fPl/jhvg3NGN5QKlzfMu04l4CcN3xatTB8jkeq/WAKeN8+MtSU8axWrFmLtq1a3fV1319ffMdWAAAcL3h8d8AAACFcF0/RAsAgNKgJD2jwgwEFwAAWMzVbkUluAAAwGLsuQAAACgEMhcAAFiMPRcAAMBUrrbngrIIAAAwFZkLAAAs5mofZUFwAQCAxbhbBAAAoBAILgAAsJjdpKMgpk6dqtq1a8vb21thYWHaunXrFftOnz5d7dq1U0BAgAICAhQeHn7V/ldCcAEAgMUMk/7Jr0WLFikqKkoxMTHauXOnmjVrpoiICCUnJ+faf/369XrkkUf01VdfafPmzapRo4Y6duyoEydO5GveYv3IdavwketA7vjIdSCnovjI9ftq3mvKOCuPr8pX/7CwMLVs2VJTpkyRJNntdtWoUUNDhgzRiBEjrnl+dna2AgICNGXKFPXp0yfP87KhEwAAi5m1oTM9PV3p6elObV5eXvLy8srRNyMjQzt27FB0dLSjzc3NTeHh4dq8eXOe5rtw4YIyMzNVsWLFfK2TsggAABYzDMOUIzY2Vv7+/k5HbGxsrnOmpqYqOztbISEhTu0hISFKTEzM07pffvllVa1aVeHh4fm6XjIXAABYzKwndEZHRysqKsqpLbeshRlef/11ffTRR1q/fr28vb3zdS7BBQAAJcSVSiC5CQwMlLu7u5KSkpzak5KSVLly5aue+9Zbb+n111/XF198oaZNm+Z7nZRFAACwWHHcLeLp6anQ0FAlJCQ42ux2uxISEtS6desrnvfGG29o3LhxWrNmjVq0aFGg6yVzAQCAxYrrCZ1RUVHq27evWrRooVatWikuLk5paWnq37+/JKlPnz6qVq2aY9/Gv//9b40aNUoLFixQ7dq1HXsz/Pz85Ofnl+d5CS4AACilevbsqZSUFI0aNUqJiYlq3ry51qxZ49jkefz4cbm5/a+IMW3aNGVkZKhHjx5O48TExGj06NF5npfnXAAuhOdcADkVxXMu7q7e0ZRxEn773JRxrEbmAgAAi/HBZQAAAIVA5gIAAIsV5HNBSjKCCwAALGYvfdsbr4qyCAAAMBWZCwAALOZaeQuCCwAALOdqd4sQXAAAYDFXCy7YcwEAAExF5gIAAIuVwodhXxXBBQAAFqMsAgAAUAhkLgAAsBhP6AQAAKZytT0XlEUAAICpyFwAAGAxV9vQSXABAIDFKIsAAAAUApkLAAAsRlkEAACYiltRAQCAqezsuQAAACg4MhcAAFiMsggAADAVZREAAIBCIHMBAIDFKIsAAABTURYBAAAoBDIXAABYjLIIAAAwFWURAACAQiBzAQCAxSiLAAAAUxmGvbiXUKQILgAAsJirfeQ6ey4AAICpyFwAAGAxw8XuFiG4AADAYpRFAAAACoHMBQAAFqMsAgAATMUTOgEAAAqBzAUAABbjCZ0AAMBUrrbngrIIAAAwFZkLAAAs5mrPuSC4AADAYq5WFiG4AADAYtyKCgAAUAhkLgAAsBhlEQAAYCpX29BJWQQAAJiKzAUAABajLAIAAEzF3SIAAACFQOYCAACL8cFlAADAVJRFAAAACoHMBQAAFuNuEQAAYCr2XAAAAFO5WuaCPRcAAMBUZC4AALCYq2UuCC4AALCYa4UWlEUAAIDJbIar5WpQZNLT0xUbG6vo6Gh5eXkV93KA6wY/GyjtCC5gmXPnzsnf319nz55V+fLli3s5wHWDnw2UdpRFAACAqQguAACAqQguAACAqQguYBkvLy/FxMSwYQ24DD8bKO3Y0AkAAExF5gIAAJiK4AIAAJiK4AIAAJiK4AIAAJiK4AKWmTp1qmrXri1vb2+FhYVp69atxb0koFh988036tq1q6pWrSqbzaZPP/20uJcEWILgApZYtGiRoqKiFBMTo507d6pZs2aKiIhQcnJycS8NKDZpaWlq1qyZpk6dWtxLASzFraiwRFhYmFq2bKkpU6ZIkux2u2rUqKEhQ4ZoxIgRxbw6oPjZbDYtW7ZMkZGRxb0UwHRkLmC6jIwM7dixQ+Hh4Y42Nzc3hYeHa/PmzcW4MgBAUSC4gOlSU1OVnZ2tkJAQp/aQkBAlJiYW06oAAEWF4AIAAJiK4AKmCwwMlLu7u5KSkpzak5KSVLly5WJaFQCgqBBcwHSenp4KDQ1VQkKCo81utyshIUGtW7cuxpUBAIpCmeJeAEqnqKgo9e3bVy1atFCrVq0UFxentLQ09e/fv7iXBhSb8+fP69ChQ46vjx49qt27d6tixYqqWbNmMa4MMBe3osIyU6ZM0ZtvvqnExEQ1b95ckydPVlhYWHEvCyg269evV4cOHXK09+3bV7Nnzy76BQEWIbgAAACmYs8FAAAwFcEFAAAwFcEFAAAwFcEFAAAwFcEFAAAwFcEFAAAwFcEFAAAwFcEFUIr069dPkZGRjq/vvPNOPf/884Ua04wxALgWggugCPTr1082m002m02enp6qV6+exo4dq6ysLEvnXbp0qcaNG5envuvXr5fNZtOZM2cKPAYASHy2CFBkOnXqpFmzZik9PV2rV6/W4MGD5eHhoejoaKd+GRkZ8vT0NGXOihUrXhdjAHAtZC6AIuLl5aXKlSurVq1aGjRokMLDw7V8+XJHKWPChAmqWrWqGjRoIEn69ddf9fDDD6tChQqqWLGiunXrpmPHjjnGy87OVlRUlCpUqKBKlSrppZde0uVP87+8pJGenq6XX35ZNWrUkJeXl+rVq6cZM2bo2LFjjs+8CAgIkM1mU79+/XId4/Tp0+rTp48CAgLk4+Ojzp076+DBg47XZ8+erQoVKmjt2rVq1KiR/Pz81KlTJ/3xxx/mvqEArlsEF0AxKVu2rDIyMiRJCQkJ2r9/v9atW6eVK1cqMzNTERERKleunL799ltt3LjR8Uv60jkTJ07U7NmzNXPmTG3YsEGnTp3SsmXLrjpnnz59tHDhQk2ePFl79+7Ve++9Jz8/P9WoUUNLliyRJO3fv19//PGH3nnnnVzH6Nevn7Zv367ly5dr8+bNMgxDXbp0UWZmpqPPhQsX9NZbb2nevHn65ptvdPz4cQ0fPtyMtw1ACUBZBChihmEoISFBa9eu1ZAhQ5SSkiJfX1998MEHjnLIhx9+KLvdrg8++EA2m02SNGvWLFWoUEHr169Xx44dFRcXp+joaD3wwAOSpPj4eK1du/aK8x44cEAff/yx1q1bp/DwcElSnTp1HK9fKn8EBwerQoUKuY5x8OBBLV++XBs3blSbNm0kSfPnz1eNGjX06aef6qGHHpIkZWZmKj4+XnXr1pUkPfvssxo7dmxB3zIAJQzBBVBEVq5cKT8/P2VmZsput6tXr14aPXq0Bg8erJtvvtlpn8X333+vQ4cOqVy5ck5jXLx4UYcPH9bZs2f1xx9/OH2EfZkyZdSiRYscpZFLdu/eLXd3d7Vv377A17B3716VKVPGad5KlSqpQYMG2rt3r6PNx8fHEVhIUpUqVZScnFzgeQGULAQXQBHp0KGDpk2bJk9PT1WtWlVlyvzvx8/X19ep7/nz5xUaGqr58+fnGCcoKKhA85ctW7ZA5xWEh4eH09c2m+2KQQ+A0oc9F0AR8fX1Vb169VSzZk2nwCI3t956qw4ePKjg4GDVq1fP6fD395e/v7+qVKmi7777znFOVlaWduzYccUxb775Ztntdn399de5vn4pc5KdnX3FMRo1aqSsrCyneU+ePKn9+/ercePGV70mAK6D4AK4Dj366KMKDAxUt27d9O233+ro0aNav369nnvuOf3222+SpKFDh+r111/Xp59+qn379umZZ57J8YyKf6pdu7b69u2rxx9/XJ9++qljzI8//liSVKtWLdlsNq1cuVIpKSk6f/58jjFuvPFGdevWTQMGDNCGDRv0/fff67HHHlO1atXUrVs3S94LACUPwQVwHfLx8dE333yjmjVr6oEHHlCjRo30xBNP6OLFiypfvrwk6YUXXlDv3r3Vt29ftW7dWuXKlVP37t2vOu60adPUo0cPPfPMM2rYsKEGDBigtLQ0SVK1atU0ZswYjRgxQiEhIXr22WdzHWPWrFkKDQ3Vfffdp9atW8swDK1evTpHKQSA67IZFEIBAICJyFwAAABTEVwAAABTEVwAAABTEVwAAABTEVwAAABTEVwAAABTEVwAAABTEVwAAABTEVwAAABTEVwAAABTEVwAAABTEVwAAABT/T8bnad5au3jQwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = predict_and_plot(X_test, test_targets, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
